{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import os\n",
    "import sys\n",
    "import traceback\n",
    "\n",
    "import torch\n",
    "\n",
    "from hydra import compose, initialize_config_module\n",
    "from hydra.utils import instantiate\n",
    "\n",
    "from omegaconf import OmegaConf, open_dict\n",
    "\n",
    "from training.utils.train_utils import makedir, register_omegaconf_resolvers\n",
    "\n",
    "os.environ[\"HYDRA_FULL_ERROR\"] = \"1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def single_proc_run(local_rank, main_port, cfg, world_size):\n",
    "    \"\"\"Single GPU process\"\"\"\n",
    "    os.environ[\"MASTER_ADDR\"] = \"localhost\"\n",
    "    os.environ[\"MASTER_PORT\"] = str(main_port)\n",
    "    os.environ[\"RANK\"] = str(local_rank)\n",
    "    os.environ[\"LOCAL_RANK\"] = str(local_rank)\n",
    "    os.environ[\"WORLD_SIZE\"] = str(world_size)\n",
    "    try:\n",
    "        register_omegaconf_resolvers()\n",
    "    except Exception as e:\n",
    "        logging.info(e)\n",
    "\n",
    "    trainer = instantiate(cfg.trainer, _recursive_=False)\n",
    "    trainer.run()\n",
    "\n",
    "\n",
    "def single_node_runner(cfg, main_port: int):\n",
    "\n",
    "    # CUDA runtime does not support `fork`\n",
    "    torch.multiprocessing.set_start_method(\"spawn\")\n",
    "\n",
    "    single_proc_run(local_rank=0, main_port=main_port, cfg=cfg, world_size=1)\n",
    "\n",
    "\n",
    "def format_exception(e: Exception, limit=20):\n",
    "    traceback_str = \"\".join(traceback.format_tb(e.__traceback__, limit=limit))\n",
    "    return f\"{type(e).__name__}: {e}\\nTraceback:\\n{traceback_str}\"\n",
    "\n",
    "\n",
    "def add_pythonpath_to_sys_path():\n",
    "    if \"PYTHONPATH\" not in os.environ or not os.environ[\"PYTHONPATH\"]:\n",
    "        return\n",
    "    sys.path = os.environ[\"PYTHONPATH\"].split(\":\") + sys.path\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "initialize_config_module(\"sam2\", version_base=\"1.2\")\n",
    "register_omegaconf_resolvers()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg = compose(config_name=\"configs/sam2.1_training/sam2.1_hiera_b+_MOSE_finetune.yaml\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add new keys\n",
    "with open_dict(cfg):\n",
    "    cfg.trainer.data.train.datasets[0].dataset.datasets[\n",
    "        0\n",
    "    ].video_dataset.is_palette = False\n",
    "    cfg.trainer.data.train.datasets[0].dataset.datasets[\n",
    "        0\n",
    "    ].video_dataset.single_object_mode = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Customize the config\n",
    "cfg.scratch.max_num_objects = 3\n",
    "cfg.scratch.num_epochs = 20\n",
    "cfg.launcher.gpus_per_node = 1\n",
    "cfg.launcher.num_nodes = 1\n",
    "cfg.dataset.img_folder = (\n",
    "    \"/home/kasm-user/sam2_ft_runpod/prepped_mini_dataset_png_fixed/images\"\n",
    ")\n",
    "cfg.dataset.gt_folder = (\n",
    "    \"/home/kasm-user/sam2_ft_runpod/prepped_mini_dataset_png_fixed/annotations\"\n",
    ")\n",
    "cfg.dataset.file_list_txt = (\n",
    "    \"/home/kasm-user/sam2_ft_runpod/prepped_mini_dataset_png_fixed/list_files.txt\"\n",
    ")\n",
    "cfg.trainer.checkpoint.model_weight_initializer.state_dict.checkpoint_path = (\n",
    "    \"/home/kasm-user/sam2_ft_runpod/checkpoints/sam2.1_hiera_base_plus.pt\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "if cfg.launcher.experiment_log_dir is None:\n",
    "    cfg.launcher.experiment_log_dir = os.path.join(\n",
    "        os.getcwd(), \"sam2_logs\", \"experiment_log_dir\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "###################### Train App Config ####################\n",
      "scratch:\n",
      "  resolution: 1024\n",
      "  train_batch_size: 1\n",
      "  num_train_workers: 10\n",
      "  num_frames: 8\n",
      "  max_num_objects: 3\n",
      "  base_lr: 5.0e-06\n",
      "  vision_lr: 3.0e-06\n",
      "  phases_per_epoch: 1\n",
      "  num_epochs: 20\n",
      "dataset:\n",
      "  img_folder: /home/kasm-user/sam2_ft_runpod/prepped_mini_dataset_png_fixed/images\n",
      "  gt_folder: /home/kasm-user/sam2_ft_runpod/prepped_mini_dataset_png_fixed/annotations\n",
      "  file_list_txt: /home/kasm-user/sam2_ft_runpod/prepped_mini_dataset_png_fixed/list_files.txt\n",
      "  multiplier: 2\n",
      "vos:\n",
      "  train_transforms:\n",
      "  - _target_: training.dataset.transforms.ComposeAPI\n",
      "    transforms:\n",
      "    - _target_: training.dataset.transforms.RandomHorizontalFlip\n",
      "      consistent_transform: true\n",
      "    - _target_: training.dataset.transforms.RandomAffine\n",
      "      degrees: 25\n",
      "      shear: 20\n",
      "      image_interpolation: bilinear\n",
      "      consistent_transform: true\n",
      "    - _target_: training.dataset.transforms.RandomResizeAPI\n",
      "      sizes: ${scratch.resolution}\n",
      "      square: true\n",
      "      consistent_transform: true\n",
      "    - _target_: training.dataset.transforms.ColorJitter\n",
      "      consistent_transform: true\n",
      "      brightness: 0.1\n",
      "      contrast: 0.03\n",
      "      saturation: 0.03\n",
      "      hue: null\n",
      "    - _target_: training.dataset.transforms.RandomGrayscale\n",
      "      p: 0.05\n",
      "      consistent_transform: true\n",
      "    - _target_: training.dataset.transforms.ColorJitter\n",
      "      consistent_transform: false\n",
      "      brightness: 0.1\n",
      "      contrast: 0.05\n",
      "      saturation: 0.05\n",
      "      hue: null\n",
      "    - _target_: training.dataset.transforms.ToTensorAPI\n",
      "    - _target_: training.dataset.transforms.NormalizeAPI\n",
      "      mean:\n",
      "      - 0.485\n",
      "      - 0.456\n",
      "      - 0.406\n",
      "      std:\n",
      "      - 0.229\n",
      "      - 0.224\n",
      "      - 0.225\n",
      "trainer:\n",
      "  _target_: training.trainer.Trainer\n",
      "  mode: train_only\n",
      "  max_epochs: ${times:${scratch.num_epochs},${scratch.phases_per_epoch}}\n",
      "  accelerator: cuda\n",
      "  seed_value: 123\n",
      "  model:\n",
      "    _target_: training.model.sam2.SAM2Train\n",
      "    image_encoder:\n",
      "      _target_: sam2.modeling.backbones.image_encoder.ImageEncoder\n",
      "      scalp: 1\n",
      "      trunk:\n",
      "        _target_: sam2.modeling.backbones.hieradet.Hiera\n",
      "        embed_dim: 112\n",
      "        num_heads: 2\n",
      "        drop_path_rate: 0.1\n",
      "      neck:\n",
      "        _target_: sam2.modeling.backbones.image_encoder.FpnNeck\n",
      "        position_encoding:\n",
      "          _target_: sam2.modeling.position_encoding.PositionEmbeddingSine\n",
      "          num_pos_feats: 256\n",
      "          normalize: true\n",
      "          scale: null\n",
      "          temperature: 10000\n",
      "        d_model: 256\n",
      "        backbone_channel_list:\n",
      "        - 896\n",
      "        - 448\n",
      "        - 224\n",
      "        - 112\n",
      "        fpn_top_down_levels:\n",
      "        - 2\n",
      "        - 3\n",
      "        fpn_interp_model: nearest\n",
      "    memory_attention:\n",
      "      _target_: sam2.modeling.memory_attention.MemoryAttention\n",
      "      d_model: 256\n",
      "      pos_enc_at_input: true\n",
      "      layer:\n",
      "        _target_: sam2.modeling.memory_attention.MemoryAttentionLayer\n",
      "        activation: relu\n",
      "        dim_feedforward: 2048\n",
      "        dropout: 0.1\n",
      "        pos_enc_at_attn: false\n",
      "        self_attention:\n",
      "          _target_: sam2.modeling.sam.transformer.RoPEAttention\n",
      "          rope_theta: 10000.0\n",
      "          feat_sizes:\n",
      "          - 32\n",
      "          - 32\n",
      "          embedding_dim: 256\n",
      "          num_heads: 1\n",
      "          downsample_rate: 1\n",
      "          dropout: 0.1\n",
      "        d_model: 256\n",
      "        pos_enc_at_cross_attn_keys: true\n",
      "        pos_enc_at_cross_attn_queries: false\n",
      "        cross_attention:\n",
      "          _target_: sam2.modeling.sam.transformer.RoPEAttention\n",
      "          rope_theta: 10000.0\n",
      "          feat_sizes:\n",
      "          - 32\n",
      "          - 32\n",
      "          rope_k_repeat: true\n",
      "          embedding_dim: 256\n",
      "          num_heads: 1\n",
      "          downsample_rate: 1\n",
      "          dropout: 0.1\n",
      "          kv_in_dim: 64\n",
      "      num_layers: 4\n",
      "    memory_encoder:\n",
      "      _target_: sam2.modeling.memory_encoder.MemoryEncoder\n",
      "      out_dim: 64\n",
      "      position_encoding:\n",
      "        _target_: sam2.modeling.position_encoding.PositionEmbeddingSine\n",
      "        num_pos_feats: 64\n",
      "        normalize: true\n",
      "        scale: null\n",
      "        temperature: 10000\n",
      "      mask_downsampler:\n",
      "        _target_: sam2.modeling.memory_encoder.MaskDownSampler\n",
      "        kernel_size: 3\n",
      "        stride: 2\n",
      "        padding: 1\n",
      "      fuser:\n",
      "        _target_: sam2.modeling.memory_encoder.Fuser\n",
      "        layer:\n",
      "          _target_: sam2.modeling.memory_encoder.CXBlock\n",
      "          dim: 256\n",
      "          kernel_size: 7\n",
      "          padding: 3\n",
      "          layer_scale_init_value: 1.0e-06\n",
      "          use_dwconv: true\n",
      "        num_layers: 2\n",
      "    num_maskmem: 7\n",
      "    image_size: ${scratch.resolution}\n",
      "    sigmoid_scale_for_mem_enc: 20.0\n",
      "    sigmoid_bias_for_mem_enc: -10.0\n",
      "    use_mask_input_as_output_without_sam: true\n",
      "    directly_add_no_mem_embed: true\n",
      "    no_obj_embed_spatial: true\n",
      "    use_high_res_features_in_sam: true\n",
      "    multimask_output_in_sam: true\n",
      "    iou_prediction_use_sigmoid: true\n",
      "    use_obj_ptrs_in_encoder: true\n",
      "    add_tpos_enc_to_obj_ptrs: true\n",
      "    proj_tpos_enc_in_obj_ptrs: true\n",
      "    use_signed_tpos_enc_to_obj_ptrs: true\n",
      "    only_obj_ptrs_in_the_past_for_eval: true\n",
      "    pred_obj_scores: true\n",
      "    pred_obj_scores_mlp: true\n",
      "    fixed_no_obj_ptr: true\n",
      "    multimask_output_for_tracking: true\n",
      "    use_multimask_token_for_obj_ptr: true\n",
      "    multimask_min_pt_num: 0\n",
      "    multimask_max_pt_num: 1\n",
      "    use_mlp_for_obj_ptr_proj: true\n",
      "    prob_to_use_pt_input_for_train: 0.5\n",
      "    prob_to_use_pt_input_for_eval: 0.0\n",
      "    prob_to_use_box_input_for_train: 0.5\n",
      "    prob_to_use_box_input_for_eval: 0.0\n",
      "    prob_to_sample_from_gt_for_train: 0.1\n",
      "    num_frames_to_correct_for_train: 2\n",
      "    num_frames_to_correct_for_eval: 1\n",
      "    rand_frames_to_correct_for_train: true\n",
      "    add_all_frames_to_correct_as_cond: true\n",
      "    num_init_cond_frames_for_train: 2\n",
      "    rand_init_cond_frames_for_train: true\n",
      "    num_correction_pt_per_frame: 7\n",
      "    use_act_ckpt_iterative_pt_sampling: false\n",
      "    num_init_cond_frames_for_eval: 1\n",
      "    forward_backbone_per_frame_for_eval: true\n",
      "  data:\n",
      "    train:\n",
      "      _target_: training.dataset.sam2_datasets.TorchTrainMixedDataset\n",
      "      phases_per_epoch: ${scratch.phases_per_epoch}\n",
      "      batch_sizes:\n",
      "      - ${scratch.train_batch_size}\n",
      "      datasets:\n",
      "      - _target_: training.dataset.utils.RepeatFactorWrapper\n",
      "        dataset:\n",
      "          _target_: training.dataset.utils.ConcatDataset\n",
      "          datasets:\n",
      "          - _target_: training.dataset.vos_dataset.VOSDataset\n",
      "            transforms: ${vos.train_transforms}\n",
      "            training: true\n",
      "            video_dataset:\n",
      "              _target_: training.dataset.vos_raw_dataset.PNGRawDataset\n",
      "              img_folder: ${dataset.img_folder}\n",
      "              gt_folder: ${dataset.gt_folder}\n",
      "              file_list_txt: ${dataset.file_list_txt}\n",
      "              is_palette: false\n",
      "              single_object_mode: true\n",
      "            sampler:\n",
      "              _target_: training.dataset.vos_sampler.RandomUniformSampler\n",
      "              num_frames: ${scratch.num_frames}\n",
      "              max_num_objects: ${scratch.max_num_objects}\n",
      "            multiplier: ${dataset.multiplier}\n",
      "      shuffle: true\n",
      "      num_workers: ${scratch.num_train_workers}\n",
      "      pin_memory: true\n",
      "      drop_last: true\n",
      "      collate_fn:\n",
      "        _target_: training.utils.data_utils.collate_fn\n",
      "        _partial_: true\n",
      "        dict_key: all\n",
      "  optim:\n",
      "    amp:\n",
      "      enabled: true\n",
      "      amp_dtype: bfloat16\n",
      "    optimizer:\n",
      "      _target_: torch.optim.AdamW\n",
      "    gradient_clip:\n",
      "      _target_: training.optimizer.GradientClipper\n",
      "      max_norm: 0.1\n",
      "      norm_type: 2\n",
      "    param_group_modifiers:\n",
      "    - _target_: training.optimizer.layer_decay_param_modifier\n",
      "      _partial_: true\n",
      "      layer_decay_value: 0.9\n",
      "      apply_to: image_encoder.trunk\n",
      "      overrides:\n",
      "      - pattern: '*pos_embed*'\n",
      "        value: 1.0\n",
      "    options:\n",
      "      lr:\n",
      "      - scheduler:\n",
      "          _target_: fvcore.common.param_scheduler.CosineParamScheduler\n",
      "          start_value: ${scratch.base_lr}\n",
      "          end_value: ${divide:${scratch.base_lr},10}\n",
      "      - scheduler:\n",
      "          _target_: fvcore.common.param_scheduler.CosineParamScheduler\n",
      "          start_value: ${scratch.vision_lr}\n",
      "          end_value: ${divide:${scratch.vision_lr},10}\n",
      "        param_names:\n",
      "        - image_encoder.*\n",
      "      weight_decay:\n",
      "      - scheduler:\n",
      "          _target_: fvcore.common.param_scheduler.ConstantParamScheduler\n",
      "          value: 0.1\n",
      "      - scheduler:\n",
      "          _target_: fvcore.common.param_scheduler.ConstantParamScheduler\n",
      "          value: 0.0\n",
      "        param_names:\n",
      "        - '*bias*'\n",
      "        module_cls_names:\n",
      "        - torch.nn.LayerNorm\n",
      "  loss:\n",
      "    all:\n",
      "      _target_: training.loss_fns.MultiStepMultiMasksAndIous\n",
      "      weight_dict:\n",
      "        loss_mask: 20\n",
      "        loss_dice: 1\n",
      "        loss_iou: 1\n",
      "        loss_class: 1\n",
      "      supervise_all_iou: true\n",
      "      iou_use_l1_loss: true\n",
      "      pred_obj_scores: true\n",
      "      focal_gamma_obj_score: 0.0\n",
      "      focal_alpha_obj_score: -1.0\n",
      "  distributed:\n",
      "    backend: nccl\n",
      "    find_unused_parameters: true\n",
      "  logging:\n",
      "    tensorboard_writer:\n",
      "      _target_: training.utils.logger.make_tensorboard_logger\n",
      "      log_dir: ${launcher.experiment_log_dir}/tensorboard\n",
      "      flush_secs: 120\n",
      "      should_log: true\n",
      "    log_dir: ${launcher.experiment_log_dir}/logs\n",
      "    log_freq: 10\n",
      "  checkpoint:\n",
      "    save_dir: ${launcher.experiment_log_dir}/checkpoints\n",
      "    save_freq: 0\n",
      "    model_weight_initializer:\n",
      "      _partial_: true\n",
      "      _target_: training.utils.checkpoint_utils.load_state_dict_into_model\n",
      "      strict: true\n",
      "      ignore_unexpected_keys: null\n",
      "      ignore_missing_keys: null\n",
      "      state_dict:\n",
      "        _target_: training.utils.checkpoint_utils.load_checkpoint_and_apply_kernels\n",
      "        checkpoint_path: /home/kasm-user/sam2_ft_runpod/checkpoints/sam2.1_hiera_base_plus.pt\n",
      "        ckpt_state_dict_keys:\n",
      "        - model\n",
      "launcher:\n",
      "  num_nodes: 1\n",
      "  gpus_per_node: 1\n",
      "  experiment_log_dir: /home/kasm-user/sam2_ft_runpod/training_sav_png_fixed/sam2_logs/experiment_log_dir\n",
      "submitit:\n",
      "  partition: null\n",
      "  account: null\n",
      "  qos: null\n",
      "  cpus_per_task: 10\n",
      "  use_cluster: false\n",
      "  timeout_hour: 24\n",
      "  name: null\n",
      "  port_range:\n",
      "  - 10000\n",
      "  - 65000\n",
      "\n",
      "############################################################\n"
     ]
    }
   ],
   "source": [
    "print(\"###################### Train App Config ####################\")\n",
    "print(OmegaConf.to_yaml(cfg))\n",
    "print(\"############################################################\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "add_pythonpath_to_sys_path()\n",
    "makedir(cfg.launcher.experiment_log_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 2025-02-12 19:05:50,831 train_utils.py: 108: MACHINE SEED: 2460\n",
      "INFO 2025-02-12 19:05:50,834 train_utils.py: 154: Logging ENV_VARIABLES\n",
      "INFO 2025-02-12 19:05:50,835 train_utils.py: 155: AUDIO_PORT=4901\n",
      "CLICOLOR=1\n",
      "CLICOLOR_FORCE=1\n",
      "COLORTERM=truecolor\n",
      "CONDA_DEFAULT_ENV=sam2_ft\n",
      "CONDA_EXE=/home/kasm-user/miniconda3/bin/conda\n",
      "CONDA_PREFIX=/home/kasm-user/miniconda3/envs/sam2_ft\n",
      "CONDA_PREFIX_1=/home/kasm-user/miniconda3\n",
      "CONDA_PROMPT_MODIFIER=(sam2_ft) \n",
      "CONDA_PYTHON_EXE=/home/kasm-user/miniconda3/bin/python\n",
      "CONDA_SHLVL=2\n",
      "CUDA_MODULE_LOADING=LAZY\n",
      "DBUS_SESSION_BUS_ADDRESS=unix:abstract=/tmp/dbus-S7FLjLiiqz,guid=19cd8a6fabda7cf5f433be6667acef54\n",
      "DEBIAN_FRONTEND=noninteractive\n",
      "DESKTOP_SESSION=xfce\n",
      "DISPLAY=:1.0\n",
      "DISTRO=ubuntu\n",
      "FORCE_COLOR=1\n",
      "GIT_PAGER=cat\n",
      "GOMP_SPINCOUNT=0\n",
      "HOME=/home/kasm-user\n",
      "HOSTNAME=51b4585ea9f4\n",
      "HYDRA_FULL_ERROR=1\n",
      "INST_SCRIPTS=/dockerstartup/install\n",
      "JPY_PARENT_PID=6505\n",
      "JPY_SESSION_NAME=/home/kasm-user/sam2_ft_runpod/training_sav_png_fixed/train.ipynb\n",
      "JUPYTER_PASSWORD=ebn2aa6noedpwv8199ao\n",
      "KASMVNC_AUTO_RECOVER=true\n",
      "KASM_VNC_PATH=/usr/share/kasmvnc\n",
      "LANG=en_US.UTF-8\n",
      "LANGUAGE=en_US:en\n",
      "LC_ALL=en_US.UTF-8\n",
      "LD_LIBRARY_PATH=/usr/lib/x86_64-linux-gnu:/usr/lib/i386-linux-gnu:/usr/local/nvidia/lib:/usr/local/nvidia/lib64\n",
      "LOCAL_RANK=0\n",
      "MASTER_ADDR=localhost\n",
      "MASTER_PORT=4500\n",
      "MAX_FRAME_RATE=24\n",
      "MPLBACKEND=module://matplotlib_inline.backend_inline\n",
      "NO_VNC_PORT=6901\n",
      "NVIDIA_DRIVER_CAPABILITIES=compute,display,graphics,utility,video\n",
      "NVIDIA_VISIBLE_DEVICES=GPU-e103b5d2-38f5-b097-e688-2313f5b343a0\n",
      "OLDPWD=/home/kasm-user\n",
      "OMP_WAIT_POLICY=PASSIVE\n",
      "PAGER=cat\n",
      "PANEL_GDK_CORE_DEVICE_EVENTS=0\n",
      "PATH=/home/kasm-user/miniconda3/envs/sam2_ft/bin:/home/kasm-user/miniconda3/condabin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin\n",
      "PS1=(sam2_ft) default:\\w$ \n",
      "PUBLIC_KEY=null\n",
      "PULSE_RUNTIME_PATH=/var/run/pulse\n",
      "PWD=/home/kasm-user/sam2_ft_runpod/training_sav_png_fixed\n",
      "PYDEVD_USE_FRAME_EVAL=NO\n",
      "RANK=0\n",
      "RUNPOD_API_KEY=rpa_NJC5IKIQBZ6ATZMTCI0MU2OLKOFUPZFNC7T1EPT11yd8z1\n",
      "RUNPOD_CPU_COUNT=31\n",
      "RUNPOD_DC_ID=CA-MTL-3\n",
      "RUNPOD_GPU_COUNT=1\n",
      "RUNPOD_GPU_NAME=NVIDIA+A100+80GB+PCIe\n",
      "RUNPOD_MEM_GB=117\n",
      "RUNPOD_POD_HOSTNAME=enrlm0fuzo092h-64410ae7\n",
      "RUNPOD_POD_ID=enrlm0fuzo092h\n",
      "SDL_GAMECONTROLLERCONFIG=030000005e040000be02000014010000,XInput Controller,platform:Linux,a:b0,b:b1,x:b2,y:b3,back:b8,guide:b16,start:b9,leftstick:b10,rightstick:b11,leftshoulder:b4,rightshoulder:b5,dpup:b12,dpdown:b13,dpleft:b14,dpright:b15,leftx:a0,lefty:a1,rightx:a2,righty:a3,lefttrigger:b6,righttrigger:b7\n",
      "SESSION_MANAGER=local/51b4585ea9f4:@/tmp/.ICE-unix/88,unix/51b4585ea9f4:/tmp/.ICE-unix/88\n",
      "SHELL=/bin/bash\n",
      "SHLVL=1\n",
      "SSH_AGENT_PID=155\n",
      "SSH_AUTH_SOCK=/tmp/ssh-bQ7Nctq4EGJC/agent.154\n",
      "STARTUPDIR=/dockerstartup\n",
      "START_PULSEAUDIO=1\n",
      "START_XFCE4=1\n",
      "TERM=xterm-color\n",
      "TORCH_NCCL_ASYNC_ERROR_HANDLING=1\n",
      "TZ=Etc/UTC\n",
      "VNCOPTIONS=-PreferBandwidth -DynamicQualityMin=4 -DynamicQualityMax=7 -DLP_ClipDelay=0 -select-de manual -UnixRelay printer:/tmp/printer\n",
      "VNC_COL_DEPTH=24\n",
      "VNC_PORT=5901\n",
      "VNC_RESOLUTION=1280x720\n",
      "VTE_VERSION=6003\n",
      "WINDOWID=31457283\n",
      "WORLD_SIZE=1\n",
      "XDG_CONFIG_DIRS=/etc/xdg\n",
      "XDG_CURRENT_DESKTOP=XFCE\n",
      "XDG_DATA_DIRS=/usr/local/share:/usr/share\n",
      "XDG_MENU_PREFIX=xfce-\n",
      "_=/home/kasm-user/miniconda3/envs/sam2_ft/bin/jupyter\n",
      "\n",
      "INFO 2025-02-12 19:05:50,836 trainer.py: 989: Setting up components: Model, loss, optim, meters etc.\n",
      "INFO 2025-02-12 19:05:50,838 logger.py:  66: TensorBoard SummaryWriter instantiated. Files will be stored in: /home/kasm-user/sam2_ft_runpod/training_sav_png_fixed/sam2_logs/experiment_log_dir/tensorboard\n",
      "INFO 2025-02-12 19:05:51,665 sam2.py:  81: Training with points (sampled from masks) as inputs with p=0.5\n",
      "INFO 2025-02-12 19:05:51,668 trainer.py:1059: ====================\n",
      "INFO 2025-02-12 19:05:51,669 trainer.py:1060: Summary for model <class 'training.model.sam2.SAM2Train'>\n",
      "INFO 2025-02-12 19:05:51,672 trainer.py:1061: Model is SAM2Train(\n",
      "  (image_encoder): ImageEncoder(\n",
      "    (trunk): Hiera(\n",
      "      (patch_embed): PatchEmbed(\n",
      "        (proj): Conv2d(3, 112, kernel_size=(7, 7), stride=(4, 4), padding=(3, 3))\n",
      "      )\n",
      "      (blocks): ModuleList(\n",
      "        (0): MultiScaleBlock(\n",
      "          (norm1): LayerNorm((112,), eps=1e-06, elementwise_affine=True)\n",
      "          (attn): MultiScaleAttention(\n",
      "            (qkv): Linear(in_features=112, out_features=336, bias=True)\n",
      "            (proj): Linear(in_features=112, out_features=112, bias=True)\n",
      "          )\n",
      "          (drop_path): Identity()\n",
      "          (norm2): LayerNorm((112,), eps=1e-06, elementwise_affine=True)\n",
      "          (mlp): MLP(\n",
      "            (layers): ModuleList(\n",
      "              (0): Linear(in_features=112, out_features=448, bias=True)\n",
      "              (1): Linear(in_features=448, out_features=112, bias=True)\n",
      "            )\n",
      "            (act): GELU(approximate='none')\n",
      "          )\n",
      "        )\n",
      "        (1): MultiScaleBlock(\n",
      "          (norm1): LayerNorm((112,), eps=1e-06, elementwise_affine=True)\n",
      "          (attn): MultiScaleAttention(\n",
      "            (qkv): Linear(in_features=112, out_features=336, bias=True)\n",
      "            (proj): Linear(in_features=112, out_features=112, bias=True)\n",
      "          )\n",
      "          (drop_path): DropPath()\n",
      "          (norm2): LayerNorm((112,), eps=1e-06, elementwise_affine=True)\n",
      "          (mlp): MLP(\n",
      "            (layers): ModuleList(\n",
      "              (0): Linear(in_features=112, out_features=448, bias=True)\n",
      "              (1): Linear(in_features=448, out_features=112, bias=True)\n",
      "            )\n",
      "            (act): GELU(approximate='none')\n",
      "          )\n",
      "        )\n",
      "        (2): MultiScaleBlock(\n",
      "          (norm1): LayerNorm((112,), eps=1e-06, elementwise_affine=True)\n",
      "          (pool): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), padding=0, dilation=1, ceil_mode=False)\n",
      "          (attn): MultiScaleAttention(\n",
      "            (q_pool): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), padding=0, dilation=1, ceil_mode=False)\n",
      "            (qkv): Linear(in_features=112, out_features=672, bias=True)\n",
      "            (proj): Linear(in_features=224, out_features=224, bias=True)\n",
      "          )\n",
      "          (drop_path): DropPath()\n",
      "          (norm2): LayerNorm((224,), eps=1e-06, elementwise_affine=True)\n",
      "          (mlp): MLP(\n",
      "            (layers): ModuleList(\n",
      "              (0): Linear(in_features=224, out_features=896, bias=True)\n",
      "              (1): Linear(in_features=896, out_features=224, bias=True)\n",
      "            )\n",
      "            (act): GELU(approximate='none')\n",
      "          )\n",
      "          (proj): Linear(in_features=112, out_features=224, bias=True)\n",
      "        )\n",
      "        (3-4): 2 x MultiScaleBlock(\n",
      "          (norm1): LayerNorm((224,), eps=1e-06, elementwise_affine=True)\n",
      "          (attn): MultiScaleAttention(\n",
      "            (qkv): Linear(in_features=224, out_features=672, bias=True)\n",
      "            (proj): Linear(in_features=224, out_features=224, bias=True)\n",
      "          )\n",
      "          (drop_path): DropPath()\n",
      "          (norm2): LayerNorm((224,), eps=1e-06, elementwise_affine=True)\n",
      "          (mlp): MLP(\n",
      "            (layers): ModuleList(\n",
      "              (0): Linear(in_features=224, out_features=896, bias=True)\n",
      "              (1): Linear(in_features=896, out_features=224, bias=True)\n",
      "            )\n",
      "            (act): GELU(approximate='none')\n",
      "          )\n",
      "        )\n",
      "        (5): MultiScaleBlock(\n",
      "          (norm1): LayerNorm((224,), eps=1e-06, elementwise_affine=True)\n",
      "          (pool): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), padding=0, dilation=1, ceil_mode=False)\n",
      "          (attn): MultiScaleAttention(\n",
      "            (q_pool): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), padding=0, dilation=1, ceil_mode=False)\n",
      "            (qkv): Linear(in_features=224, out_features=1344, bias=True)\n",
      "            (proj): Linear(in_features=448, out_features=448, bias=True)\n",
      "          )\n",
      "          (drop_path): DropPath()\n",
      "          (norm2): LayerNorm((448,), eps=1e-06, elementwise_affine=True)\n",
      "          (mlp): MLP(\n",
      "            (layers): ModuleList(\n",
      "              (0): Linear(in_features=448, out_features=1792, bias=True)\n",
      "              (1): Linear(in_features=1792, out_features=448, bias=True)\n",
      "            )\n",
      "            (act): GELU(approximate='none')\n",
      "          )\n",
      "          (proj): Linear(in_features=224, out_features=448, bias=True)\n",
      "        )\n",
      "        (6-20): 15 x MultiScaleBlock(\n",
      "          (norm1): LayerNorm((448,), eps=1e-06, elementwise_affine=True)\n",
      "          (attn): MultiScaleAttention(\n",
      "            (qkv): Linear(in_features=448, out_features=1344, bias=True)\n",
      "            (proj): Linear(in_features=448, out_features=448, bias=True)\n",
      "          )\n",
      "          (drop_path): DropPath()\n",
      "          (norm2): LayerNorm((448,), eps=1e-06, elementwise_affine=True)\n",
      "          (mlp): MLP(\n",
      "            (layers): ModuleList(\n",
      "              (0): Linear(in_features=448, out_features=1792, bias=True)\n",
      "              (1): Linear(in_features=1792, out_features=448, bias=True)\n",
      "            )\n",
      "            (act): GELU(approximate='none')\n",
      "          )\n",
      "        )\n",
      "        (21): MultiScaleBlock(\n",
      "          (norm1): LayerNorm((448,), eps=1e-06, elementwise_affine=True)\n",
      "          (pool): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), padding=0, dilation=1, ceil_mode=False)\n",
      "          (attn): MultiScaleAttention(\n",
      "            (q_pool): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), padding=0, dilation=1, ceil_mode=False)\n",
      "            (qkv): Linear(in_features=448, out_features=2688, bias=True)\n",
      "            (proj): Linear(in_features=896, out_features=896, bias=True)\n",
      "          )\n",
      "          (drop_path): DropPath()\n",
      "          (norm2): LayerNorm((896,), eps=1e-06, elementwise_affine=True)\n",
      "          (mlp): MLP(\n",
      "            (layers): ModuleList(\n",
      "              (0): Linear(in_features=896, out_features=3584, bias=True)\n",
      "              (1): Linear(in_features=3584, out_features=896, bias=True)\n",
      "            )\n",
      "            (act): GELU(approximate='none')\n",
      "          )\n",
      "          (proj): Linear(in_features=448, out_features=896, bias=True)\n",
      "        )\n",
      "        (22-23): 2 x MultiScaleBlock(\n",
      "          (norm1): LayerNorm((896,), eps=1e-06, elementwise_affine=True)\n",
      "          (attn): MultiScaleAttention(\n",
      "            (qkv): Linear(in_features=896, out_features=2688, bias=True)\n",
      "            (proj): Linear(in_features=896, out_features=896, bias=True)\n",
      "          )\n",
      "          (drop_path): DropPath()\n",
      "          (norm2): LayerNorm((896,), eps=1e-06, elementwise_affine=True)\n",
      "          (mlp): MLP(\n",
      "            (layers): ModuleList(\n",
      "              (0): Linear(in_features=896, out_features=3584, bias=True)\n",
      "              (1): Linear(in_features=3584, out_features=896, bias=True)\n",
      "            )\n",
      "            (act): GELU(approximate='none')\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (neck): FpnNeck(\n",
      "      (position_encoding): PositionEmbeddingSine()\n",
      "      (convs): ModuleList(\n",
      "        (0): Sequential(\n",
      "          (conv): Conv2d(896, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "        )\n",
      "        (1): Sequential(\n",
      "          (conv): Conv2d(448, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "        )\n",
      "        (2): Sequential(\n",
      "          (conv): Conv2d(224, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "        )\n",
      "        (3): Sequential(\n",
      "          (conv): Conv2d(112, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (mask_downsample): Conv2d(1, 1, kernel_size=(4, 4), stride=(4, 4))\n",
      "  (memory_attention): MemoryAttention(\n",
      "    (layers): ModuleList(\n",
      "      (0-3): 4 x MemoryAttentionLayer(\n",
      "        (self_attn): RoPEAttention(\n",
      "          (q_proj): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (k_proj): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (v_proj): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (out_proj): Linear(in_features=256, out_features=256, bias=True)\n",
      "        )\n",
      "        (cross_attn_image): RoPEAttention(\n",
      "          (q_proj): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (k_proj): Linear(in_features=64, out_features=256, bias=True)\n",
      "          (v_proj): Linear(in_features=64, out_features=256, bias=True)\n",
      "          (out_proj): Linear(in_features=256, out_features=256, bias=True)\n",
      "        )\n",
      "        (linear1): Linear(in_features=256, out_features=2048, bias=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "        (linear2): Linear(in_features=2048, out_features=256, bias=True)\n",
      "        (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (dropout1): Dropout(p=0.1, inplace=False)\n",
      "        (dropout2): Dropout(p=0.1, inplace=False)\n",
      "        (dropout3): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "  )\n",
      "  (memory_encoder): MemoryEncoder(\n",
      "    (mask_downsampler): MaskDownSampler(\n",
      "      (encoder): Sequential(\n",
      "        (0): Conv2d(1, 4, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
      "        (1): LayerNorm2d()\n",
      "        (2): GELU(approximate='none')\n",
      "        (3): Conv2d(4, 16, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
      "        (4): LayerNorm2d()\n",
      "        (5): GELU(approximate='none')\n",
      "        (6): Conv2d(16, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
      "        (7): LayerNorm2d()\n",
      "        (8): GELU(approximate='none')\n",
      "        (9): Conv2d(64, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
      "        (10): LayerNorm2d()\n",
      "        (11): GELU(approximate='none')\n",
      "        (12): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "      )\n",
      "    )\n",
      "    (pix_feat_proj): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "    (fuser): Fuser(\n",
      "      (proj): Identity()\n",
      "      (layers): ModuleList(\n",
      "        (0-1): 2 x CXBlock(\n",
      "          (dwconv): Conv2d(256, 256, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=256)\n",
      "          (norm): LayerNorm2d()\n",
      "          (pwconv1): Linear(in_features=256, out_features=1024, bias=True)\n",
      "          (act): GELU(approximate='none')\n",
      "          (pwconv2): Linear(in_features=1024, out_features=256, bias=True)\n",
      "          (drop_path): Identity()\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (position_encoding): PositionEmbeddingSine()\n",
      "    (out_proj): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1))\n",
      "  )\n",
      "  (sam_prompt_encoder): PromptEncoder(\n",
      "    (pe_layer): PositionEmbeddingRandom()\n",
      "    (point_embeddings): ModuleList(\n",
      "      (0-3): 4 x Embedding(1, 256)\n",
      "    )\n",
      "    (not_a_point_embed): Embedding(1, 256)\n",
      "    (mask_downscaling): Sequential(\n",
      "      (0): Conv2d(1, 4, kernel_size=(2, 2), stride=(2, 2))\n",
      "      (1): LayerNorm2d()\n",
      "      (2): GELU(approximate='none')\n",
      "      (3): Conv2d(4, 16, kernel_size=(2, 2), stride=(2, 2))\n",
      "      (4): LayerNorm2d()\n",
      "      (5): GELU(approximate='none')\n",
      "      (6): Conv2d(16, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "    )\n",
      "    (no_mask_embed): Embedding(1, 256)\n",
      "  )\n",
      "  (sam_mask_decoder): MaskDecoder(\n",
      "    (transformer): TwoWayTransformer(\n",
      "      (layers): ModuleList(\n",
      "        (0-1): 2 x TwoWayAttentionBlock(\n",
      "          (self_attn): Attention(\n",
      "            (q_proj): Linear(in_features=256, out_features=256, bias=True)\n",
      "            (k_proj): Linear(in_features=256, out_features=256, bias=True)\n",
      "            (v_proj): Linear(in_features=256, out_features=256, bias=True)\n",
      "            (out_proj): Linear(in_features=256, out_features=256, bias=True)\n",
      "          )\n",
      "          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "          (cross_attn_token_to_image): Attention(\n",
      "            (q_proj): Linear(in_features=256, out_features=128, bias=True)\n",
      "            (k_proj): Linear(in_features=256, out_features=128, bias=True)\n",
      "            (v_proj): Linear(in_features=256, out_features=128, bias=True)\n",
      "            (out_proj): Linear(in_features=128, out_features=256, bias=True)\n",
      "          )\n",
      "          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "          (mlp): MLP(\n",
      "            (layers): ModuleList(\n",
      "              (0): Linear(in_features=256, out_features=2048, bias=True)\n",
      "              (1): Linear(in_features=2048, out_features=256, bias=True)\n",
      "            )\n",
      "            (act): ReLU()\n",
      "          )\n",
      "          (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "          (norm4): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "          (cross_attn_image_to_token): Attention(\n",
      "            (q_proj): Linear(in_features=256, out_features=128, bias=True)\n",
      "            (k_proj): Linear(in_features=256, out_features=128, bias=True)\n",
      "            (v_proj): Linear(in_features=256, out_features=128, bias=True)\n",
      "            (out_proj): Linear(in_features=128, out_features=256, bias=True)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (final_attn_token_to_image): Attention(\n",
      "        (q_proj): Linear(in_features=256, out_features=128, bias=True)\n",
      "        (k_proj): Linear(in_features=256, out_features=128, bias=True)\n",
      "        (v_proj): Linear(in_features=256, out_features=128, bias=True)\n",
      "        (out_proj): Linear(in_features=128, out_features=256, bias=True)\n",
      "      )\n",
      "      (norm_final_attn): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "    )\n",
      "    (iou_token): Embedding(1, 256)\n",
      "    (mask_tokens): Embedding(4, 256)\n",
      "    (obj_score_token): Embedding(1, 256)\n",
      "    (output_upscaling): Sequential(\n",
      "      (0): ConvTranspose2d(256, 64, kernel_size=(2, 2), stride=(2, 2))\n",
      "      (1): LayerNorm2d()\n",
      "      (2): GELU(approximate='none')\n",
      "      (3): ConvTranspose2d(64, 32, kernel_size=(2, 2), stride=(2, 2))\n",
      "      (4): GELU(approximate='none')\n",
      "    )\n",
      "    (conv_s0): Conv2d(256, 32, kernel_size=(1, 1), stride=(1, 1))\n",
      "    (conv_s1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1))\n",
      "    (output_hypernetworks_mlps): ModuleList(\n",
      "      (0-3): 4 x MLP(\n",
      "        (layers): ModuleList(\n",
      "          (0-1): 2 x Linear(in_features=256, out_features=256, bias=True)\n",
      "          (2): Linear(in_features=256, out_features=32, bias=True)\n",
      "        )\n",
      "        (act): ReLU()\n",
      "      )\n",
      "    )\n",
      "    (iou_prediction_head): MLP(\n",
      "      (layers): ModuleList(\n",
      "        (0-1): 2 x Linear(in_features=256, out_features=256, bias=True)\n",
      "        (2): Linear(in_features=256, out_features=4, bias=True)\n",
      "      )\n",
      "      (act): ReLU()\n",
      "    )\n",
      "    (pred_obj_score_head): MLP(\n",
      "      (layers): ModuleList(\n",
      "        (0-1): 2 x Linear(in_features=256, out_features=256, bias=True)\n",
      "        (2): Linear(in_features=256, out_features=1, bias=True)\n",
      "      )\n",
      "      (act): ReLU()\n",
      "    )\n",
      "  )\n",
      "  (obj_ptr_proj): MLP(\n",
      "    (layers): ModuleList(\n",
      "      (0-2): 3 x Linear(in_features=256, out_features=256, bias=True)\n",
      "    )\n",
      "    (act): ReLU()\n",
      "  )\n",
      "  (obj_ptr_tpos_proj): Linear(in_features=256, out_features=64, bias=True)\n",
      ")\n",
      "INFO 2025-02-12 19:05:51,672 trainer.py:1062: \tTotal parameters 80.9 M\n",
      "INFO 2025-02-12 19:05:51,673 trainer.py:1063: \tTrainable parameters 80.9 M\n",
      "INFO 2025-02-12 19:05:51,673 trainer.py:1066: \tNon-Trainable parameters 0  \n",
      "INFO 2025-02-12 19:05:51,674 trainer.py:1069: ====================\n",
      "INFO 2025-02-12 19:05:51,678 trainer.py:1023: Finished setting up components: Model, loss, optim, meters etc.\n",
      "INFO 2025-02-12 19:05:51,679 trainer.py: 314: Moving components to device cuda:0 and local rank 0.\n",
      "INFO 2025-02-12 19:05:51,935 trainer.py: 320: Done moving components to device cuda:0 and local rank 0.\n",
      "INFO 2025-02-12 19:05:51,949 optimizer.py: 248: Matches for param_name [image_encoder.*]: {'image_encoder.trunk.blocks.11.mlp.layers.0.bias', 'image_encoder.trunk.blocks.20.attn.qkv.weight', 'image_encoder.trunk.blocks.1.norm1.bias', 'image_encoder.trunk.blocks.11.attn.proj.weight', 'image_encoder.trunk.blocks.1.attn.qkv.weight', 'image_encoder.trunk.blocks.23.norm1.weight', 'image_encoder.trunk.blocks.1.mlp.layers.1.weight', 'image_encoder.trunk.blocks.7.mlp.layers.1.weight', 'image_encoder.trunk.blocks.23.attn.qkv.bias', 'image_encoder.trunk.blocks.3.norm1.weight', 'image_encoder.trunk.blocks.18.mlp.layers.0.weight', 'image_encoder.trunk.patch_embed.proj.weight', 'image_encoder.trunk.blocks.5.attn.proj.weight', 'image_encoder.trunk.blocks.23.norm1.bias', 'image_encoder.trunk.blocks.12.norm2.bias', 'image_encoder.trunk.blocks.19.mlp.layers.0.bias', 'image_encoder.trunk.blocks.9.attn.proj.weight', 'image_encoder.trunk.blocks.3.attn.qkv.bias', 'image_encoder.trunk.pos_embed', 'image_encoder.trunk.blocks.5.attn.qkv.weight', 'image_encoder.trunk.blocks.5.attn.proj.bias', 'image_encoder.trunk.blocks.0.mlp.layers.0.weight', 'image_encoder.trunk.patch_embed.proj.bias', 'image_encoder.trunk.blocks.9.norm1.weight', 'image_encoder.trunk.blocks.4.attn.proj.bias', 'image_encoder.trunk.blocks.6.attn.proj.weight', 'image_encoder.neck.convs.1.conv.weight', 'image_encoder.trunk.blocks.6.norm1.bias', 'image_encoder.trunk.blocks.10.attn.qkv.weight', 'image_encoder.trunk.blocks.21.mlp.layers.0.bias', 'image_encoder.trunk.blocks.4.mlp.layers.0.bias', 'image_encoder.trunk.blocks.15.mlp.layers.1.weight', 'image_encoder.trunk.blocks.15.norm2.weight', 'image_encoder.trunk.blocks.12.mlp.layers.1.weight', 'image_encoder.trunk.blocks.0.norm1.bias', 'image_encoder.trunk.blocks.20.mlp.layers.1.bias', 'image_encoder.trunk.blocks.17.norm2.weight', 'image_encoder.trunk.blocks.20.norm2.weight', 'image_encoder.trunk.blocks.23.mlp.layers.0.weight', 'image_encoder.trunk.blocks.1.norm2.bias', 'image_encoder.trunk.blocks.15.norm1.bias', 'image_encoder.trunk.blocks.16.attn.qkv.weight', 'image_encoder.trunk.blocks.2.proj.weight', 'image_encoder.trunk.blocks.13.attn.proj.bias', 'image_encoder.trunk.blocks.9.mlp.layers.1.weight', 'image_encoder.trunk.blocks.17.attn.qkv.weight', 'image_encoder.trunk.blocks.5.mlp.layers.1.bias', 'image_encoder.trunk.blocks.12.norm1.weight', 'image_encoder.trunk.blocks.7.mlp.layers.0.bias', 'image_encoder.trunk.blocks.10.mlp.layers.1.weight', 'image_encoder.trunk.blocks.14.mlp.layers.1.bias', 'image_encoder.trunk.blocks.22.mlp.layers.0.weight', 'image_encoder.trunk.blocks.22.attn.qkv.weight', 'image_encoder.trunk.blocks.9.mlp.layers.0.bias', 'image_encoder.trunk.blocks.2.proj.bias', 'image_encoder.trunk.blocks.2.mlp.layers.0.weight', 'image_encoder.trunk.blocks.16.attn.proj.weight', 'image_encoder.trunk.blocks.3.attn.qkv.weight', 'image_encoder.trunk.blocks.11.attn.qkv.bias', 'image_encoder.trunk.blocks.20.attn.proj.bias', 'image_encoder.trunk.blocks.12.attn.qkv.bias', 'image_encoder.trunk.blocks.20.mlp.layers.1.weight', 'image_encoder.trunk.blocks.18.norm2.weight', 'image_encoder.trunk.blocks.4.mlp.layers.1.weight', 'image_encoder.trunk.blocks.15.mlp.layers.0.bias', 'image_encoder.trunk.blocks.20.norm1.weight', 'image_encoder.trunk.blocks.0.norm2.weight', 'image_encoder.trunk.blocks.11.norm1.bias', 'image_encoder.trunk.blocks.5.proj.weight', 'image_encoder.trunk.blocks.4.norm1.bias', 'image_encoder.trunk.blocks.12.norm1.bias', 'image_encoder.trunk.blocks.19.norm2.bias', 'image_encoder.trunk.blocks.19.norm1.bias', 'image_encoder.trunk.blocks.8.attn.proj.weight', 'image_encoder.trunk.blocks.1.attn.qkv.bias', 'image_encoder.trunk.blocks.9.norm2.bias', 'image_encoder.trunk.blocks.5.mlp.layers.0.bias', 'image_encoder.trunk.blocks.16.mlp.layers.0.weight', 'image_encoder.neck.convs.2.conv.weight', 'image_encoder.trunk.blocks.10.norm2.weight', 'image_encoder.trunk.blocks.0.mlp.layers.1.bias', 'image_encoder.trunk.blocks.15.norm1.weight', 'image_encoder.trunk.blocks.0.norm2.bias', 'image_encoder.trunk.blocks.13.mlp.layers.0.weight', 'image_encoder.trunk.blocks.5.attn.qkv.bias', 'image_encoder.trunk.blocks.16.attn.proj.bias', 'image_encoder.trunk.blocks.12.attn.qkv.weight', 'image_encoder.trunk.blocks.2.mlp.layers.1.bias', 'image_encoder.trunk.blocks.18.mlp.layers.1.bias', 'image_encoder.trunk.blocks.23.mlp.layers.0.bias', 'image_encoder.trunk.blocks.16.mlp.layers.1.weight', 'image_encoder.trunk.blocks.10.mlp.layers.0.weight', 'image_encoder.trunk.blocks.7.norm1.weight', 'image_encoder.trunk.blocks.14.norm2.weight', 'image_encoder.trunk.blocks.12.attn.proj.bias', 'image_encoder.trunk.blocks.11.mlp.layers.1.weight', 'image_encoder.trunk.blocks.14.norm1.bias', 'image_encoder.trunk.blocks.14.mlp.layers.0.weight', 'image_encoder.trunk.blocks.22.norm1.bias', 'image_encoder.trunk.blocks.7.norm1.bias', 'image_encoder.trunk.blocks.19.attn.proj.bias', 'image_encoder.trunk.blocks.2.mlp.layers.1.weight', 'image_encoder.trunk.blocks.0.attn.qkv.bias', 'image_encoder.trunk.blocks.14.norm1.weight', 'image_encoder.neck.convs.0.conv.weight', 'image_encoder.trunk.blocks.10.norm1.bias', 'image_encoder.trunk.blocks.9.attn.qkv.bias', 'image_encoder.trunk.blocks.5.norm1.weight', 'image_encoder.trunk.blocks.8.mlp.layers.1.weight', 'image_encoder.trunk.blocks.23.attn.qkv.weight', 'image_encoder.trunk.blocks.17.mlp.layers.1.bias', 'image_encoder.trunk.blocks.6.mlp.layers.1.weight', 'image_encoder.trunk.blocks.10.mlp.layers.1.bias', 'image_encoder.trunk.blocks.8.mlp.layers.1.bias', 'image_encoder.trunk.blocks.3.norm1.bias', 'image_encoder.trunk.blocks.15.mlp.layers.1.bias', 'image_encoder.trunk.blocks.7.attn.qkv.weight', 'image_encoder.trunk.blocks.15.attn.qkv.bias', 'image_encoder.trunk.blocks.9.norm1.bias', 'image_encoder.trunk.blocks.4.attn.qkv.bias', 'image_encoder.trunk.blocks.10.attn.proj.bias', 'image_encoder.trunk.blocks.4.norm1.weight', 'image_encoder.neck.convs.3.conv.bias', 'image_encoder.trunk.blocks.1.mlp.layers.0.weight', 'image_encoder.trunk.blocks.5.mlp.layers.0.weight', 'image_encoder.trunk.blocks.1.mlp.layers.1.bias', 'image_encoder.trunk.blocks.21.norm1.bias', 'image_encoder.trunk.blocks.5.norm2.bias', 'image_encoder.trunk.blocks.16.norm2.bias', 'image_encoder.trunk.blocks.9.mlp.layers.1.bias', 'image_encoder.trunk.blocks.16.attn.qkv.bias', 'image_encoder.trunk.blocks.7.mlp.layers.1.bias', 'image_encoder.trunk.blocks.12.norm2.weight', 'image_encoder.trunk.blocks.11.attn.proj.bias', 'image_encoder.trunk.blocks.22.mlp.layers.0.bias', 'image_encoder.trunk.blocks.17.norm1.weight', 'image_encoder.trunk.blocks.13.norm2.bias', 'image_encoder.trunk.blocks.9.mlp.layers.0.weight', 'image_encoder.trunk.blocks.4.norm2.bias', 'image_encoder.trunk.blocks.6.attn.qkv.weight', 'image_encoder.trunk.blocks.6.norm2.bias', 'image_encoder.trunk.blocks.13.attn.proj.weight', 'image_encoder.trunk.blocks.11.norm2.weight', 'image_encoder.trunk.blocks.0.mlp.layers.1.weight', 'image_encoder.trunk.blocks.21.norm2.bias', 'image_encoder.trunk.blocks.3.attn.proj.weight', 'image_encoder.trunk.blocks.21.proj.weight', 'image_encoder.trunk.blocks.1.attn.proj.bias', 'image_encoder.trunk.blocks.21.norm2.weight', 'image_encoder.trunk.blocks.8.mlp.layers.0.weight', 'image_encoder.trunk.blocks.20.mlp.layers.0.weight', 'image_encoder.trunk.blocks.0.norm1.weight', 'image_encoder.trunk.blocks.21.attn.qkv.bias', 'image_encoder.trunk.blocks.0.mlp.layers.0.bias', 'image_encoder.trunk.blocks.20.norm1.bias', 'image_encoder.trunk.blocks.16.norm2.weight', 'image_encoder.trunk.blocks.17.attn.proj.bias', 'image_encoder.trunk.blocks.17.mlp.layers.0.bias', 'image_encoder.trunk.blocks.11.attn.qkv.weight', 'image_encoder.trunk.blocks.19.mlp.layers.1.weight', 'image_encoder.trunk.blocks.23.mlp.layers.1.weight', 'image_encoder.trunk.blocks.2.norm2.bias', 'image_encoder.trunk.blocks.6.mlp.layers.0.bias', 'image_encoder.trunk.blocks.1.attn.proj.weight', 'image_encoder.trunk.blocks.6.norm1.weight', 'image_encoder.trunk.blocks.6.mlp.layers.1.bias', 'image_encoder.trunk.blocks.18.norm1.bias', 'image_encoder.trunk.blocks.10.norm2.bias', 'image_encoder.trunk.blocks.13.attn.qkv.weight', 'image_encoder.trunk.blocks.5.mlp.layers.1.weight', 'image_encoder.trunk.blocks.17.attn.proj.weight', 'image_encoder.trunk.blocks.13.norm1.weight', 'image_encoder.trunk.blocks.18.attn.proj.bias', 'image_encoder.trunk.blocks.19.mlp.layers.0.weight', 'image_encoder.trunk.blocks.3.attn.proj.bias', 'image_encoder.trunk.blocks.18.attn.qkv.bias', 'image_encoder.trunk.blocks.7.norm2.weight', 'image_encoder.trunk.blocks.15.norm2.bias', 'image_encoder.trunk.blocks.15.mlp.layers.0.weight', 'image_encoder.trunk.pos_embed_window', 'image_encoder.trunk.blocks.7.mlp.layers.0.weight', 'image_encoder.trunk.blocks.3.mlp.layers.1.weight', 'image_encoder.trunk.blocks.7.attn.qkv.bias', 'image_encoder.trunk.blocks.8.attn.qkv.weight', 'image_encoder.trunk.blocks.8.norm1.weight', 'image_encoder.trunk.blocks.10.mlp.layers.0.bias', 'image_encoder.trunk.blocks.3.mlp.layers.1.bias', 'image_encoder.trunk.blocks.8.mlp.layers.0.bias', 'image_encoder.trunk.blocks.16.mlp.layers.1.bias', 'image_encoder.trunk.blocks.14.attn.proj.bias', 'image_encoder.trunk.blocks.5.norm2.weight', 'image_encoder.trunk.blocks.15.attn.proj.bias', 'image_encoder.trunk.blocks.4.mlp.layers.1.bias', 'image_encoder.trunk.blocks.17.mlp.layers.1.weight', 'image_encoder.trunk.blocks.15.attn.proj.weight', 'image_encoder.trunk.blocks.7.norm2.bias', 'image_encoder.trunk.blocks.3.norm2.weight', 'image_encoder.trunk.blocks.2.norm1.bias', 'image_encoder.trunk.blocks.1.norm1.weight', 'image_encoder.trunk.blocks.12.mlp.layers.1.bias', 'image_encoder.trunk.blocks.11.mlp.layers.1.bias', 'image_encoder.trunk.blocks.7.attn.proj.bias', 'image_encoder.trunk.blocks.5.norm1.bias', 'image_encoder.trunk.blocks.3.mlp.layers.0.weight', 'image_encoder.trunk.blocks.18.attn.proj.weight', 'image_encoder.trunk.blocks.22.norm1.weight', 'image_encoder.trunk.blocks.23.attn.proj.weight', 'image_encoder.trunk.blocks.2.attn.qkv.bias', 'image_encoder.trunk.blocks.2.norm2.weight', 'image_encoder.trunk.blocks.4.attn.proj.weight', 'image_encoder.trunk.blocks.6.attn.qkv.bias', 'image_encoder.trunk.blocks.12.mlp.layers.0.weight', 'image_encoder.trunk.blocks.7.attn.proj.weight', 'image_encoder.trunk.blocks.16.norm1.bias', 'image_encoder.trunk.blocks.13.mlp.layers.1.weight', 'image_encoder.trunk.blocks.21.attn.qkv.weight', 'image_encoder.trunk.blocks.21.proj.bias', 'image_encoder.trunk.blocks.10.norm1.weight', 'image_encoder.trunk.blocks.13.mlp.layers.0.bias', 'image_encoder.trunk.blocks.11.mlp.layers.0.weight', 'image_encoder.trunk.blocks.13.mlp.layers.1.bias', 'image_encoder.trunk.blocks.14.norm2.bias', 'image_encoder.trunk.blocks.23.mlp.layers.1.bias', 'image_encoder.trunk.blocks.17.norm1.bias', 'image_encoder.trunk.blocks.1.norm2.weight', 'image_encoder.trunk.blocks.12.mlp.layers.0.bias', 'image_encoder.trunk.blocks.22.attn.qkv.bias', 'image_encoder.trunk.blocks.2.norm1.weight', 'image_encoder.trunk.blocks.8.norm2.weight', 'image_encoder.trunk.blocks.17.norm2.bias', 'image_encoder.trunk.blocks.22.attn.proj.weight', 'image_encoder.trunk.blocks.0.attn.proj.weight', 'image_encoder.trunk.blocks.14.mlp.layers.1.weight', 'image_encoder.trunk.blocks.22.norm2.bias', 'image_encoder.trunk.blocks.4.norm2.weight', 'image_encoder.trunk.blocks.5.proj.bias', 'image_encoder.trunk.blocks.10.attn.proj.weight', 'image_encoder.trunk.blocks.4.mlp.layers.0.weight', 'image_encoder.trunk.blocks.11.norm2.bias', 'image_encoder.trunk.blocks.23.attn.proj.bias', 'image_encoder.trunk.blocks.11.norm1.weight', 'image_encoder.trunk.blocks.19.attn.qkv.weight', 'image_encoder.trunk.blocks.20.attn.qkv.bias', 'image_encoder.trunk.blocks.20.mlp.layers.0.bias', 'image_encoder.trunk.blocks.23.norm2.bias', 'image_encoder.trunk.blocks.2.attn.proj.weight', 'image_encoder.trunk.blocks.14.attn.qkv.bias', 'image_encoder.trunk.blocks.6.mlp.layers.0.weight', 'image_encoder.trunk.blocks.18.mlp.layers.0.bias', 'image_encoder.trunk.blocks.18.mlp.layers.1.weight', 'image_encoder.trunk.blocks.14.attn.qkv.weight', 'image_encoder.trunk.blocks.19.attn.qkv.bias', 'image_encoder.trunk.blocks.19.attn.proj.weight', 'image_encoder.neck.convs.0.conv.bias', 'image_encoder.trunk.blocks.2.attn.proj.bias', 'image_encoder.trunk.blocks.12.attn.proj.weight', 'image_encoder.trunk.blocks.16.norm1.weight', 'image_encoder.trunk.blocks.17.mlp.layers.0.weight', 'image_encoder.trunk.blocks.18.attn.qkv.weight', 'image_encoder.trunk.blocks.22.attn.proj.bias', 'image_encoder.trunk.blocks.20.attn.proj.weight', 'image_encoder.trunk.blocks.6.norm2.weight', 'image_encoder.neck.convs.2.conv.bias', 'image_encoder.trunk.blocks.8.attn.proj.bias', 'image_encoder.trunk.blocks.9.attn.qkv.weight', 'image_encoder.trunk.blocks.17.attn.qkv.bias', 'image_encoder.trunk.blocks.0.attn.proj.bias', 'image_encoder.neck.convs.1.conv.bias', 'image_encoder.trunk.blocks.9.attn.proj.bias', 'image_encoder.trunk.blocks.2.mlp.layers.0.bias', 'image_encoder.trunk.blocks.20.norm2.bias', 'image_encoder.trunk.blocks.0.attn.qkv.weight', 'image_encoder.trunk.blocks.21.mlp.layers.1.weight', 'image_encoder.neck.convs.3.conv.weight', 'image_encoder.trunk.blocks.3.norm2.bias', 'image_encoder.trunk.blocks.4.attn.qkv.weight', 'image_encoder.trunk.blocks.21.mlp.layers.1.bias', 'image_encoder.trunk.blocks.16.mlp.layers.0.bias', 'image_encoder.trunk.blocks.14.attn.proj.weight', 'image_encoder.trunk.blocks.10.attn.qkv.bias', 'image_encoder.trunk.blocks.19.norm1.weight', 'image_encoder.trunk.blocks.21.mlp.layers.0.weight', 'image_encoder.trunk.blocks.8.norm1.bias', 'image_encoder.trunk.blocks.8.attn.qkv.bias', 'image_encoder.trunk.blocks.21.norm1.weight', 'image_encoder.trunk.blocks.22.mlp.layers.1.bias', 'image_encoder.trunk.blocks.14.mlp.layers.0.bias', 'image_encoder.trunk.blocks.18.norm2.bias', 'image_encoder.trunk.blocks.15.attn.qkv.weight', 'image_encoder.trunk.blocks.13.norm2.weight', 'image_encoder.trunk.blocks.21.attn.proj.bias', 'image_encoder.trunk.blocks.13.attn.qkv.bias', 'image_encoder.trunk.blocks.22.norm2.weight', 'image_encoder.trunk.blocks.9.norm2.weight', 'image_encoder.trunk.blocks.2.attn.qkv.weight', 'image_encoder.trunk.blocks.18.norm1.weight', 'image_encoder.trunk.blocks.6.attn.proj.bias', 'image_encoder.trunk.blocks.22.mlp.layers.1.weight', 'image_encoder.trunk.blocks.23.norm2.weight', 'image_encoder.trunk.blocks.21.attn.proj.weight', 'image_encoder.trunk.blocks.8.norm2.bias', 'image_encoder.trunk.blocks.1.mlp.layers.0.bias', 'image_encoder.trunk.blocks.13.norm1.bias', 'image_encoder.trunk.blocks.19.norm2.weight', 'image_encoder.trunk.blocks.19.mlp.layers.1.bias', 'image_encoder.trunk.blocks.3.mlp.layers.0.bias'}\n",
      "INFO 2025-02-12 19:05:51,951 optimizer.py: 248: Matches for param_name [*bias*]: {'image_encoder.trunk.blocks.11.mlp.layers.0.bias', 'memory_attention.layers.0.self_attn.k_proj.bias', 'sam_mask_decoder.transformer.layers.1.norm1.bias', 'sam_mask_decoder.transformer.layers.1.self_attn.out_proj.bias', 'image_encoder.trunk.blocks.1.norm1.bias', 'memory_encoder.mask_downsampler.encoder.7.bias', 'memory_attention.layers.3.cross_attn_image.v_proj.bias', 'sam_mask_decoder.output_upscaling.3.bias', 'memory_attention.layers.1.cross_attn_image.q_proj.bias', 'memory_encoder.pix_feat_proj.bias', 'memory_encoder.fuser.layers.1.norm.bias', 'sam_mask_decoder.iou_prediction_head.layers.0.bias', 'memory_attention.layers.0.linear2.bias', 'image_encoder.trunk.blocks.23.attn.qkv.bias', 'image_encoder.trunk.blocks.23.norm1.bias', 'image_encoder.trunk.blocks.12.norm2.bias', 'image_encoder.trunk.blocks.19.mlp.layers.0.bias', 'sam_mask_decoder.transformer.layers.0.self_attn.v_proj.bias', 'memory_attention.layers.1.self_attn.out_proj.bias', 'image_encoder.trunk.blocks.3.attn.qkv.bias', 'sam_mask_decoder.transformer.layers.0.cross_attn_token_to_image.q_proj.bias', 'image_encoder.trunk.blocks.5.attn.proj.bias', 'memory_attention.layers.1.cross_attn_image.out_proj.bias', 'image_encoder.trunk.patch_embed.proj.bias', 'sam_mask_decoder.transformer.layers.1.norm4.bias', 'sam_mask_decoder.output_hypernetworks_mlps.1.layers.2.bias', 'memory_attention.layers.0.cross_attn_image.v_proj.bias', 'image_encoder.trunk.blocks.4.attn.proj.bias', 'memory_attention.layers.3.cross_attn_image.out_proj.bias', 'memory_attention.layers.1.norm1.bias', 'image_encoder.trunk.blocks.6.norm1.bias', 'memory_attention.layers.3.cross_attn_image.q_proj.bias', 'sam_mask_decoder.output_hypernetworks_mlps.2.layers.0.bias', 'image_encoder.trunk.blocks.21.mlp.layers.0.bias', 'image_encoder.trunk.blocks.4.mlp.layers.0.bias', 'memory_encoder.out_proj.bias', 'memory_attention.layers.2.self_attn.q_proj.bias', 'sam_mask_decoder.transformer.layers.0.self_attn.k_proj.bias', 'sam_mask_decoder.iou_prediction_head.layers.2.bias', 'memory_encoder.mask_downsampler.encoder.0.bias', 'image_encoder.trunk.blocks.0.norm1.bias', 'image_encoder.trunk.blocks.20.mlp.layers.1.bias', 'image_encoder.trunk.blocks.1.norm2.bias', 'image_encoder.trunk.blocks.15.norm1.bias', 'memory_attention.layers.0.norm2.bias', 'image_encoder.trunk.blocks.13.attn.proj.bias', 'memory_attention.layers.3.linear2.bias', 'image_encoder.trunk.blocks.5.mlp.layers.1.bias', 'sam_mask_decoder.transformer.final_attn_token_to_image.q_proj.bias', 'image_encoder.trunk.blocks.7.mlp.layers.0.bias', 'sam_prompt_encoder.mask_downscaling.3.bias', 'sam_mask_decoder.transformer.layers.1.norm3.bias', 'image_encoder.trunk.blocks.14.mlp.layers.1.bias', 'image_encoder.trunk.blocks.9.mlp.layers.0.bias', 'image_encoder.trunk.blocks.2.proj.bias', 'memory_encoder.fuser.layers.0.pwconv1.bias', 'sam_mask_decoder.transformer.layers.0.cross_attn_image_to_token.v_proj.bias', 'sam_mask_decoder.transformer.layers.1.cross_attn_image_to_token.k_proj.bias', 'sam_mask_decoder.pred_obj_score_head.layers.2.bias', 'sam_mask_decoder.iou_prediction_head.layers.1.bias', 'memory_attention.layers.2.linear1.bias', 'image_encoder.trunk.blocks.11.attn.qkv.bias', 'image_encoder.trunk.blocks.20.attn.proj.bias', 'memory_attention.layers.3.self_attn.out_proj.bias', 'image_encoder.trunk.blocks.12.attn.qkv.bias', 'sam_mask_decoder.transformer.final_attn_token_to_image.out_proj.bias', 'memory_attention.layers.2.cross_attn_image.v_proj.bias', 'image_encoder.trunk.blocks.15.mlp.layers.0.bias', 'image_encoder.trunk.blocks.11.norm1.bias', 'memory_attention.layers.2.self_attn.v_proj.bias', 'sam_mask_decoder.transformer.layers.1.self_attn.k_proj.bias', 'image_encoder.trunk.blocks.4.norm1.bias', 'image_encoder.trunk.blocks.12.norm1.bias', 'image_encoder.trunk.blocks.19.norm2.bias', 'image_encoder.trunk.blocks.19.norm1.bias', 'image_encoder.trunk.blocks.5.mlp.layers.0.bias', 'image_encoder.trunk.blocks.1.attn.qkv.bias', 'memory_attention.layers.2.norm2.bias', 'memory_attention.layers.0.self_attn.q_proj.bias', 'image_encoder.trunk.blocks.21.attn.qkv.bias', 'obj_ptr_tpos_proj.bias', 'image_encoder.trunk.blocks.0.mlp.layers.1.bias', 'image_encoder.trunk.blocks.2.mlp.layers.1.bias', 'image_encoder.trunk.blocks.0.norm2.bias', 'memory_encoder.fuser.layers.1.dwconv.bias', 'image_encoder.trunk.blocks.5.attn.qkv.bias', 'image_encoder.trunk.blocks.16.attn.proj.bias', 'memory_attention.layers.2.cross_attn_image.k_proj.bias', 'sam_mask_decoder.output_hypernetworks_mlps.2.layers.1.bias', 'image_encoder.trunk.blocks.23.mlp.layers.0.bias', 'image_encoder.trunk.blocks.18.mlp.layers.1.bias', 'sam_mask_decoder.output_hypernetworks_mlps.3.layers.2.bias', 'memory_attention.layers.2.linear2.bias', 'sam_mask_decoder.transformer.layers.0.self_attn.q_proj.bias', 'image_encoder.trunk.blocks.12.attn.proj.bias', 'memory_attention.layers.1.cross_attn_image.v_proj.bias', 'image_encoder.trunk.blocks.14.norm1.bias', 'image_encoder.trunk.blocks.22.norm1.bias', 'memory_encoder.mask_downsampler.encoder.12.bias', 'image_encoder.trunk.blocks.7.norm1.bias', 'image_encoder.trunk.blocks.19.attn.proj.bias', 'sam_mask_decoder.conv_s1.bias', 'image_encoder.trunk.blocks.0.attn.qkv.bias', 'image_encoder.trunk.blocks.10.norm1.bias', 'sam_mask_decoder.output_hypernetworks_mlps.2.layers.2.bias', 'sam_mask_decoder.output_hypernetworks_mlps.0.layers.2.bias', 'image_encoder.trunk.blocks.9.attn.qkv.bias', 'sam_mask_decoder.conv_s0.bias', 'sam_mask_decoder.transformer.layers.1.cross_attn_token_to_image.q_proj.bias', 'image_encoder.trunk.blocks.17.mlp.layers.1.bias', 'sam_mask_decoder.output_hypernetworks_mlps.3.layers.1.bias', 'image_encoder.trunk.blocks.10.mlp.layers.1.bias', 'sam_mask_decoder.transformer.layers.1.cross_attn_token_to_image.k_proj.bias', 'image_encoder.trunk.blocks.8.mlp.layers.1.bias', 'sam_mask_decoder.transformer.layers.1.mlp.layers.1.bias', 'sam_mask_decoder.output_hypernetworks_mlps.1.layers.0.bias', 'image_encoder.trunk.blocks.3.norm1.bias', 'sam_mask_decoder.transformer.layers.1.cross_attn_token_to_image.v_proj.bias', 'image_encoder.trunk.blocks.15.mlp.layers.1.bias', 'image_encoder.trunk.blocks.15.attn.qkv.bias', 'image_encoder.trunk.blocks.9.norm1.bias', 'image_encoder.trunk.blocks.4.attn.qkv.bias', 'image_encoder.trunk.blocks.10.attn.proj.bias', 'image_encoder.neck.convs.3.conv.bias', 'image_encoder.trunk.blocks.1.mlp.layers.1.bias', 'sam_prompt_encoder.mask_downscaling.6.bias', 'memory_encoder.mask_downsampler.encoder.1.bias', 'memory_encoder.mask_downsampler.encoder.6.bias', 'image_encoder.trunk.blocks.21.norm1.bias', 'image_encoder.trunk.blocks.5.norm2.bias', 'image_encoder.trunk.blocks.16.norm2.bias', 'memory_encoder.fuser.layers.1.pwconv1.bias', 'image_encoder.trunk.blocks.9.mlp.layers.1.bias', 'image_encoder.trunk.blocks.7.mlp.layers.1.bias', 'image_encoder.trunk.blocks.16.attn.qkv.bias', 'memory_attention.layers.1.self_attn.v_proj.bias', 'image_encoder.trunk.blocks.11.attn.proj.bias', 'image_encoder.trunk.blocks.22.mlp.layers.0.bias', 'image_encoder.trunk.blocks.13.norm2.bias', 'memory_attention.layers.1.linear2.bias', 'image_encoder.trunk.blocks.4.norm2.bias', 'image_encoder.trunk.blocks.6.norm2.bias', 'obj_ptr_proj.layers.1.bias', 'image_encoder.trunk.blocks.21.norm2.bias', 'image_encoder.trunk.blocks.1.attn.proj.bias', 'sam_mask_decoder.transformer.layers.1.cross_attn_image_to_token.out_proj.bias', 'memory_attention.layers.3.self_attn.q_proj.bias', 'sam_mask_decoder.transformer.layers.0.norm3.bias', 'memory_attention.layers.0.cross_attn_image.q_proj.bias', 'memory_attention.layers.3.norm1.bias', 'image_encoder.trunk.blocks.0.mlp.layers.0.bias', 'image_encoder.trunk.blocks.20.norm1.bias', 'image_encoder.trunk.blocks.17.attn.proj.bias', 'image_encoder.trunk.blocks.17.mlp.layers.0.bias', 'image_encoder.trunk.blocks.2.norm2.bias', 'image_encoder.trunk.blocks.6.mlp.layers.0.bias', 'image_encoder.trunk.blocks.9.norm2.bias', 'memory_encoder.fuser.layers.0.pwconv2.bias', 'image_encoder.trunk.blocks.10.attn.qkv.bias', 'image_encoder.trunk.blocks.10.norm2.bias', 'image_encoder.trunk.blocks.6.mlp.layers.1.bias', 'image_encoder.trunk.blocks.18.norm1.bias', 'memory_attention.layers.1.self_attn.k_proj.bias', 'memory_encoder.mask_downsampler.encoder.10.bias', 'image_encoder.trunk.blocks.18.attn.proj.bias', 'memory_attention.layers.2.cross_attn_image.out_proj.bias', 'image_encoder.trunk.blocks.18.attn.qkv.bias', 'image_encoder.trunk.blocks.3.attn.proj.bias', 'sam_mask_decoder.pred_obj_score_head.layers.1.bias', 'sam_mask_decoder.transformer.layers.1.mlp.layers.0.bias', 'sam_prompt_encoder.mask_downscaling.1.bias', 'memory_attention.layers.2.norm3.bias', 'image_encoder.trunk.blocks.15.norm2.bias', 'sam_mask_decoder.transformer.layers.1.self_attn.q_proj.bias', 'memory_attention.layers.3.self_attn.v_proj.bias', 'memory_attention.layers.0.norm1.bias', 'image_encoder.trunk.blocks.7.attn.qkv.bias', 'image_encoder.trunk.blocks.10.mlp.layers.0.bias', 'memory_attention.layers.2.self_attn.k_proj.bias', 'image_encoder.trunk.blocks.3.mlp.layers.1.bias', 'sam_mask_decoder.transformer.layers.0.mlp.layers.1.bias', 'image_encoder.trunk.blocks.8.mlp.layers.0.bias', 'memory_attention.layers.0.cross_attn_image.out_proj.bias', 'image_encoder.trunk.blocks.16.mlp.layers.1.bias', 'image_encoder.trunk.blocks.14.attn.proj.bias', 'image_encoder.trunk.blocks.15.attn.proj.bias', 'image_encoder.trunk.blocks.4.mlp.layers.1.bias', 'sam_prompt_encoder.mask_downscaling.0.bias', 'mask_downsample.bias', 'memory_attention.layers.2.norm1.bias', 'image_encoder.trunk.blocks.7.norm2.bias', 'sam_mask_decoder.output_hypernetworks_mlps.0.layers.1.bias', 'memory_attention.norm.bias', 'sam_mask_decoder.output_hypernetworks_mlps.1.layers.1.bias', 'obj_ptr_proj.layers.2.bias', 'memory_attention.layers.0.self_attn.v_proj.bias', 'sam_mask_decoder.transformer.norm_final_attn.bias', 'image_encoder.trunk.blocks.2.norm1.bias', 'sam_prompt_encoder.mask_downscaling.4.bias', 'image_encoder.trunk.blocks.12.mlp.layers.1.bias', 'memory_encoder.fuser.layers.1.pwconv2.bias', 'image_encoder.trunk.blocks.11.mlp.layers.1.bias', 'image_encoder.trunk.blocks.7.attn.proj.bias', 'memory_attention.layers.0.linear1.bias', 'memory_encoder.mask_downsampler.encoder.3.bias', 'image_encoder.trunk.blocks.5.norm1.bias', 'obj_ptr_proj.layers.0.bias', 'sam_mask_decoder.transformer.layers.1.norm2.bias', 'memory_attention.layers.2.self_attn.out_proj.bias', 'image_encoder.trunk.blocks.2.attn.qkv.bias', 'image_encoder.trunk.blocks.6.attn.qkv.bias', 'image_encoder.trunk.blocks.16.norm1.bias', 'sam_mask_decoder.transformer.layers.0.cross_attn_image_to_token.out_proj.bias', 'image_encoder.trunk.blocks.21.proj.bias', 'sam_mask_decoder.transformer.layers.1.self_attn.v_proj.bias', 'image_encoder.trunk.blocks.13.mlp.layers.0.bias', 'sam_mask_decoder.transformer.layers.0.norm4.bias', 'image_encoder.trunk.blocks.13.mlp.layers.1.bias', 'memory_attention.layers.0.self_attn.out_proj.bias', 'sam_mask_decoder.transformer.final_attn_token_to_image.v_proj.bias', 'image_encoder.trunk.blocks.14.norm2.bias', 'image_encoder.trunk.blocks.23.mlp.layers.1.bias', 'image_encoder.trunk.blocks.17.norm1.bias', 'image_encoder.trunk.blocks.12.mlp.layers.0.bias', 'image_encoder.trunk.blocks.22.attn.qkv.bias', 'memory_attention.layers.1.norm2.bias', 'sam_mask_decoder.transformer.layers.0.cross_attn_token_to_image.v_proj.bias', 'image_encoder.trunk.blocks.17.norm2.bias', 'sam_mask_decoder.transformer.layers.0.cross_attn_image_to_token.q_proj.bias', 'memory_attention.layers.2.cross_attn_image.q_proj.bias', 'memory_attention.layers.3.self_attn.k_proj.bias', 'memory_attention.layers.3.norm2.bias', 'image_encoder.trunk.blocks.22.norm2.bias', 'memory_attention.layers.1.self_attn.q_proj.bias', 'sam_mask_decoder.transformer.layers.0.cross_attn_token_to_image.k_proj.bias', 'image_encoder.trunk.blocks.5.proj.bias', 'image_encoder.trunk.blocks.11.norm2.bias', 'sam_mask_decoder.transformer.layers.1.cross_attn_image_to_token.v_proj.bias', 'image_encoder.trunk.blocks.23.attn.proj.bias', 'image_encoder.trunk.blocks.20.attn.qkv.bias', 'image_encoder.trunk.blocks.20.mlp.layers.0.bias', 'image_encoder.trunk.blocks.23.norm2.bias', 'memory_encoder.fuser.layers.0.norm.bias', 'image_encoder.trunk.blocks.18.mlp.layers.0.bias', 'image_encoder.trunk.blocks.14.attn.qkv.bias', 'sam_mask_decoder.pred_obj_score_head.layers.0.bias', 'sam_mask_decoder.transformer.final_attn_token_to_image.k_proj.bias', 'sam_mask_decoder.output_hypernetworks_mlps.3.layers.0.bias', 'sam_mask_decoder.output_upscaling.1.bias', 'image_encoder.trunk.blocks.19.attn.qkv.bias', 'memory_attention.layers.0.cross_attn_image.k_proj.bias', 'image_encoder.neck.convs.0.conv.bias', 'image_encoder.trunk.blocks.2.attn.proj.bias', 'sam_mask_decoder.transformer.layers.0.norm1.bias', 'sam_mask_decoder.transformer.layers.1.cross_attn_token_to_image.out_proj.bias', 'image_encoder.trunk.blocks.22.attn.proj.bias', 'sam_mask_decoder.transformer.layers.0.self_attn.out_proj.bias', 'memory_attention.layers.3.linear1.bias', 'memory_encoder.fuser.layers.0.dwconv.bias', 'image_encoder.neck.convs.2.conv.bias', 'memory_attention.layers.1.cross_attn_image.k_proj.bias', 'sam_mask_decoder.output_hypernetworks_mlps.0.layers.0.bias', 'image_encoder.trunk.blocks.8.attn.proj.bias', 'memory_attention.layers.3.norm3.bias', 'image_encoder.trunk.blocks.17.attn.qkv.bias', 'image_encoder.trunk.blocks.0.attn.proj.bias', 'image_encoder.neck.convs.1.conv.bias', 'image_encoder.trunk.blocks.9.attn.proj.bias', 'image_encoder.trunk.blocks.2.mlp.layers.0.bias', 'memory_attention.layers.1.norm3.bias', 'memory_encoder.mask_downsampler.encoder.4.bias', 'image_encoder.trunk.blocks.20.norm2.bias', 'image_encoder.trunk.blocks.3.norm2.bias', 'sam_mask_decoder.transformer.layers.0.cross_attn_token_to_image.out_proj.bias', 'image_encoder.trunk.blocks.21.mlp.layers.1.bias', 'image_encoder.trunk.blocks.16.mlp.layers.0.bias', 'memory_attention.layers.1.linear1.bias', 'image_encoder.trunk.blocks.8.norm1.bias', 'memory_attention.layers.3.cross_attn_image.k_proj.bias', 'image_encoder.trunk.blocks.8.attn.qkv.bias', 'memory_attention.layers.0.norm3.bias', 'image_encoder.trunk.blocks.22.mlp.layers.1.bias', 'image_encoder.trunk.blocks.14.mlp.layers.0.bias', 'image_encoder.trunk.blocks.18.norm2.bias', 'sam_mask_decoder.transformer.layers.1.cross_attn_image_to_token.q_proj.bias', 'image_encoder.trunk.blocks.21.attn.proj.bias', 'image_encoder.trunk.blocks.13.attn.qkv.bias', 'image_encoder.trunk.blocks.6.attn.proj.bias', 'sam_mask_decoder.transformer.layers.0.mlp.layers.0.bias', 'image_encoder.trunk.blocks.8.norm2.bias', 'sam_mask_decoder.output_upscaling.0.bias', 'image_encoder.trunk.blocks.1.mlp.layers.0.bias', 'sam_mask_decoder.transformer.layers.0.norm2.bias', 'memory_encoder.mask_downsampler.encoder.9.bias', 'sam_mask_decoder.transformer.layers.0.cross_attn_image_to_token.k_proj.bias', 'image_encoder.trunk.blocks.13.norm1.bias', 'image_encoder.trunk.blocks.19.mlp.layers.1.bias', 'image_encoder.trunk.blocks.3.mlp.layers.0.bias'}\n",
      "INFO 2025-02-12 19:05:51,951 optimizer.py: 220: Matches for module_cls_name [torch.nn.LayerNorm]: {'sam_mask_decoder.transformer.layers.1.norm1.bias', 'image_encoder.trunk.blocks.1.norm1.bias', 'sam_mask_decoder.transformer.layers.1.norm1.weight', 'image_encoder.trunk.blocks.23.norm1.weight', 'image_encoder.trunk.blocks.3.norm1.weight', 'image_encoder.trunk.blocks.23.norm1.bias', 'image_encoder.trunk.blocks.12.norm2.bias', 'image_encoder.trunk.blocks.9.norm1.weight', 'memory_attention.layers.1.norm1.bias', 'sam_mask_decoder.transformer.layers.1.norm4.bias', 'sam_mask_decoder.transformer.layers.1.norm4.weight', 'image_encoder.trunk.blocks.6.norm1.bias', 'image_encoder.trunk.blocks.15.norm2.weight', 'image_encoder.trunk.blocks.0.norm1.bias', 'image_encoder.trunk.blocks.17.norm2.weight', 'image_encoder.trunk.blocks.20.norm2.weight', 'image_encoder.trunk.blocks.1.norm2.bias', 'image_encoder.trunk.blocks.15.norm1.bias', 'memory_attention.layers.3.norm3.weight', 'memory_attention.layers.0.norm2.bias', 'image_encoder.trunk.blocks.12.norm1.weight', 'sam_mask_decoder.transformer.layers.0.norm1.weight', 'sam_mask_decoder.transformer.layers.1.norm3.bias', 'memory_attention.layers.2.norm2.weight', 'image_encoder.trunk.blocks.18.norm2.weight', 'image_encoder.trunk.blocks.11.norm1.bias', 'image_encoder.trunk.blocks.0.norm2.weight', 'image_encoder.trunk.blocks.20.norm1.weight', 'image_encoder.trunk.blocks.12.norm1.bias', 'image_encoder.trunk.blocks.4.norm1.bias', 'image_encoder.trunk.blocks.19.norm1.bias', 'image_encoder.trunk.blocks.19.norm2.bias', 'memory_attention.layers.2.norm2.bias', 'image_encoder.trunk.blocks.10.norm2.weight', 'image_encoder.trunk.blocks.15.norm1.weight', 'image_encoder.trunk.blocks.0.norm2.bias', 'memory_attention.layers.1.norm1.weight', 'image_encoder.trunk.blocks.7.norm1.weight', 'image_encoder.trunk.blocks.14.norm2.weight', 'memory_attention.layers.3.norm2.weight', 'image_encoder.trunk.blocks.14.norm1.bias', 'image_encoder.trunk.blocks.22.norm1.bias', 'image_encoder.trunk.blocks.7.norm1.bias', 'image_encoder.trunk.blocks.14.norm1.weight', 'image_encoder.trunk.blocks.10.norm1.bias', 'image_encoder.trunk.blocks.5.norm1.weight', 'sam_mask_decoder.transformer.layers.0.norm2.weight', 'memory_attention.layers.0.norm2.weight', 'image_encoder.trunk.blocks.3.norm1.bias', 'image_encoder.trunk.blocks.9.norm1.bias', 'memory_attention.layers.1.norm2.weight', 'image_encoder.trunk.blocks.4.norm1.weight', 'image_encoder.trunk.blocks.21.norm1.bias', 'image_encoder.trunk.blocks.5.norm2.bias', 'image_encoder.trunk.blocks.16.norm2.bias', 'sam_mask_decoder.transformer.norm_final_attn.weight', 'image_encoder.trunk.blocks.12.norm2.weight', 'image_encoder.trunk.blocks.17.norm1.weight', 'image_encoder.trunk.blocks.13.norm2.bias', 'image_encoder.trunk.blocks.4.norm2.bias', 'image_encoder.trunk.blocks.6.norm2.bias', 'image_encoder.trunk.blocks.11.norm2.weight', 'image_encoder.trunk.blocks.21.norm2.bias', 'memory_attention.layers.1.norm3.weight', 'image_encoder.trunk.blocks.21.norm2.weight', 'sam_mask_decoder.transformer.layers.0.norm3.bias', 'image_encoder.trunk.blocks.0.norm1.weight', 'memory_attention.layers.0.norm1.weight', 'image_encoder.trunk.blocks.2.norm2.bias', 'image_encoder.trunk.blocks.9.norm2.bias', 'image_encoder.trunk.blocks.16.norm2.weight', 'image_encoder.trunk.blocks.20.norm1.bias', 'memory_attention.layers.2.norm1.weight', 'image_encoder.trunk.blocks.10.norm2.bias', 'memory_attention.layers.3.norm1.bias', 'sam_mask_decoder.transformer.layers.1.norm3.weight', 'image_encoder.trunk.blocks.6.norm1.weight', 'image_encoder.trunk.blocks.18.norm1.bias', 'image_encoder.trunk.blocks.13.norm1.weight', 'memory_attention.layers.2.norm3.bias', 'image_encoder.trunk.blocks.7.norm2.weight', 'image_encoder.trunk.blocks.15.norm2.bias', 'memory_attention.layers.0.norm1.bias', 'image_encoder.trunk.blocks.8.norm1.weight', 'image_encoder.trunk.blocks.5.norm2.weight', 'sam_mask_decoder.transformer.layers.1.norm2.weight', 'image_encoder.trunk.blocks.7.norm2.bias', 'memory_attention.layers.2.norm1.bias', 'memory_attention.norm.bias', 'image_encoder.trunk.blocks.3.norm2.weight', 'image_encoder.trunk.blocks.2.norm1.bias', 'sam_mask_decoder.transformer.norm_final_attn.bias', 'image_encoder.trunk.blocks.1.norm1.weight', 'sam_mask_decoder.transformer.layers.1.norm2.bias', 'image_encoder.trunk.blocks.5.norm1.bias', 'image_encoder.trunk.blocks.22.norm1.weight', 'image_encoder.trunk.blocks.2.norm2.weight', 'image_encoder.trunk.blocks.16.norm1.bias', 'image_encoder.trunk.blocks.10.norm1.weight', 'sam_mask_decoder.transformer.layers.0.norm4.bias', 'image_encoder.trunk.blocks.14.norm2.bias', 'memory_attention.layers.3.norm1.weight', 'image_encoder.trunk.blocks.17.norm1.bias', 'image_encoder.trunk.blocks.1.norm2.weight', 'image_encoder.trunk.blocks.2.norm1.weight', 'image_encoder.trunk.blocks.8.norm2.weight', 'memory_attention.layers.1.norm2.bias', 'image_encoder.trunk.blocks.17.norm2.bias', 'memory_attention.layers.2.norm3.weight', 'memory_attention.layers.3.norm2.bias', 'image_encoder.trunk.blocks.22.norm2.bias', 'image_encoder.trunk.blocks.4.norm2.weight', 'image_encoder.trunk.blocks.11.norm2.bias', 'image_encoder.trunk.blocks.11.norm1.weight', 'image_encoder.trunk.blocks.23.norm2.bias', 'sam_mask_decoder.transformer.layers.0.norm4.weight', 'image_encoder.trunk.blocks.16.norm1.weight', 'sam_mask_decoder.transformer.layers.0.norm1.bias', 'image_encoder.trunk.blocks.6.norm2.weight', 'memory_attention.layers.3.norm3.bias', 'image_encoder.trunk.blocks.20.norm2.bias', 'memory_attention.layers.1.norm3.bias', 'image_encoder.trunk.blocks.3.norm2.bias', 'memory_attention.layers.0.norm3.weight', 'image_encoder.trunk.blocks.19.norm1.weight', 'image_encoder.trunk.blocks.8.norm1.bias', 'memory_attention.layers.0.norm3.bias', 'image_encoder.trunk.blocks.21.norm1.weight', 'image_encoder.trunk.blocks.22.norm2.weight', 'image_encoder.trunk.blocks.18.norm2.bias', 'image_encoder.trunk.blocks.9.norm2.weight', 'image_encoder.trunk.blocks.13.norm2.weight', 'sam_mask_decoder.transformer.layers.0.norm3.weight', 'image_encoder.trunk.blocks.18.norm1.weight', 'image_encoder.trunk.blocks.23.norm2.weight', 'image_encoder.trunk.blocks.8.norm2.bias', 'memory_attention.norm.weight', 'sam_mask_decoder.transformer.layers.0.norm2.bias', 'image_encoder.trunk.blocks.13.norm1.bias', 'image_encoder.trunk.blocks.19.norm2.weight'} \n",
      "Raw dataset length = 23\n",
      "INFO 2025-02-12 19:05:52,264 sam2_datasets.py: 125: Dataset mixing probabilities: [1.0]\n",
      "INFO 2025-02-12 19:05:52,529 trainer.py: 417: Loading pretrained checkpoint from {'_partial_': True, '_target_': 'training.utils.checkpoint_utils.load_state_dict_into_model', 'strict': True, 'ignore_unexpected_keys': None, 'ignore_missing_keys': None, 'state_dict': {'_target_': 'training.utils.checkpoint_utils.load_checkpoint_and_apply_kernels', 'checkpoint_path': '/home/kasm-user/sam2_ft_runpod/checkpoints/sam2.1_hiera_base_plus.pt', 'ckpt_state_dict_keys': ['model']}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Skip RandomAffine for zero-area mask in first frame after 1 tentatives\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 2025-02-12 19:06:01,606 train_utils.py: 271: Train Epoch: [0][ 0/46] | Batch Time: 8.75 (8.75) | Data Time: 5.83 (5.83) | Mem (GB): 27.00 (27.00/27.00) | Time Elapsed: 00d 00h 00m | Losses/train_all_loss: 3.49e-01 (3.49e-01)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kasm-user/miniconda3/envs/sam2_ft/lib/python3.10/site-packages/torch/autograd/graph.py:823: UserWarning: Grad strides do not match bucket view strides. This may indicate grad was not created according to the gradient layout contract, or that the param's strides changed since DDP was constructed.  This is not an error, but may impair performance.\n",
      "grad.sizes() = [64, 256, 1, 1], strides() = [256, 1, 256, 256]\n",
      "bucket_view.sizes() = [64, 256, 1, 1], strides() = [256, 1, 1, 1] (Triggered internally at /pytorch/torch/csrc/distributed/c10d/reducer.cpp:327.)\n",
      "  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 2025-02-12 19:06:10,953 train_utils.py: 271: Train Epoch: [0][10/46] | Batch Time: 0.97 (1.65) | Data Time: 0.00 (0.53) | Mem (GB): 29.00 (28.45/30.00) | Time Elapsed: 00d 00h 00m | Losses/train_all_loss: 1.56e+00 (1.13e+00)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Skip RandomAffine for zero-area mask in first frame after 1 tentatives\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 2025-02-12 19:06:19,565 train_utils.py: 271: Train Epoch: [0][20/46] | Batch Time: 0.81 (1.27) | Data Time: 0.00 (0.28) | Mem (GB): 28.00 (28.43/30.00) | Time Elapsed: 00d 00h 00m | Losses/train_all_loss: 2.96e-01 (1.15e+00)\n",
      "INFO 2025-02-12 19:06:29,407 train_utils.py: 271: Train Epoch: [0][30/46] | Batch Time: 0.80 (1.18) | Data Time: 0.00 (0.19) | Mem (GB): 28.00 (28.58/30.00) | Time Elapsed: 00d 00h 00m | Losses/train_all_loss: 2.20e-01 (1.28e+00)\n",
      "INFO 2025-02-12 19:06:38,739 train_utils.py: 271: Train Epoch: [0][40/46] | Batch Time: 0.78 (1.12) | Data Time: 0.00 (0.14) | Mem (GB): 28.00 (28.59/30.00) | Time Elapsed: 00d 00h 00m | Losses/train_all_loss: 5.56e-01 (1.50e+00)\n",
      "INFO 2025-02-12 19:06:43,982 trainer.py: 950: Estimated time remaining: 00d 00h 15m\n",
      "INFO 2025-02-12 19:06:43,984 trainer.py: 892: Synchronizing meters\n",
      "INFO 2025-02-12 19:06:43,985 trainer.py: 830: Losses and meters: {'Losses/train_all_loss': 1.4013368252502836, 'Losses/train_all_loss_mask': 0.01010155483480044, 'Losses/train_all_loss_dice': 0.5872440986011339, 'Losses/train_all_loss_iou': 0.30211484614435746, 'Losses/train_all_loss_class': 0.30994679095289507, 'Losses/train_all_core_loss': 1.4013368252502836, 'Trainer/where': 0.04891304347826087, 'Trainer/epoch': 0, 'Trainer/steps_train': 46}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Skip RandomAffine for zero-area mask in first frame after 1 tentatives\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 2025-02-12 19:06:50,776 train_utils.py: 271: Train Epoch: [1][ 0/46] | Batch Time: 5.50 (5.50) | Data Time: 4.68 (4.68) | Mem (GB): 28.00 (28.00/28.00) | Time Elapsed: 00d 00h 00m | Losses/train_all_loss: 5.38e-01 (5.38e-01)\n",
      "INFO 2025-02-12 19:07:00,627 train_utils.py: 271: Train Epoch: [1][10/46] | Batch Time: 1.14 (1.40) | Data Time: 0.00 (0.43) | Mem (GB): 29.00 (28.55/30.00) | Time Elapsed: 00d 00h 01m | Losses/train_all_loss: 1.26e+00 (1.52e+00)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Skip RandomAffine for zero-area mask in first frame after 1 tentatives\n",
      "WARNING:root:Skip RandomAffine for zero-area mask in first frame after 1 tentatives\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 2025-02-12 19:07:09,601 train_utils.py: 271: Train Epoch: [1][20/46] | Batch Time: 0.79 (1.16) | Data Time: 0.00 (0.22) | Mem (GB): 28.00 (28.48/30.00) | Time Elapsed: 00d 00h 01m | Losses/train_all_loss: 3.24e-01 (1.32e+00)\n",
      "INFO 2025-02-12 19:07:18,547 train_utils.py: 271: Train Epoch: [1][30/46] | Batch Time: 0.79 (1.07) | Data Time: 0.00 (0.15) | Mem (GB): 28.00 (28.52/30.00) | Time Elapsed: 00d 00h 01m | Losses/train_all_loss: 8.08e-02 (1.12e+00)\n",
      "INFO 2025-02-12 19:07:27,388 train_utils.py: 271: Train Epoch: [1][40/46] | Batch Time: 0.78 (1.03) | Data Time: 0.00 (0.12) | Mem (GB): 28.00 (28.54/30.00) | Time Elapsed: 00d 00h 01m | Losses/train_all_loss: 1.34e-01 (1.34e+00)\n",
      "INFO 2025-02-12 19:07:33,499 trainer.py: 950: Estimated time remaining: 00d 00h 14m\n",
      "INFO 2025-02-12 19:07:33,501 trainer.py: 892: Synchronizing meters\n",
      "INFO 2025-02-12 19:07:33,502 trainer.py: 830: Losses and meters: {'Losses/train_all_loss': 1.2975476899224778, 'Losses/train_all_loss_mask': 0.008814393642673835, 'Losses/train_all_loss_dice': 0.5514968737311985, 'Losses/train_all_loss_iou': 0.2729906533723292, 'Losses/train_all_loss_class': 0.29677228710642856, 'Losses/train_all_core_loss': 1.2975476899224778, 'Trainer/where': 0.09891304347826087, 'Trainer/epoch': 1, 'Trainer/steps_train': 92}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Skip RandomAffine for zero-area mask in first frame after 1 tentatives\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 2025-02-12 19:07:41,742 train_utils.py: 271: Train Epoch: [2][ 0/46] | Batch Time: 6.78 (6.78) | Data Time: 5.97 (5.97) | Mem (GB): 28.00 (28.00/28.00) | Time Elapsed: 00d 00h 01m | Losses/train_all_loss: 1.19e-01 (1.19e-01)\n",
      "INFO 2025-02-12 19:07:50,629 train_utils.py: 271: Train Epoch: [2][10/46] | Batch Time: 1.00 (1.42) | Data Time: 0.00 (0.54) | Mem (GB): 29.00 (28.45/30.00) | Time Elapsed: 00d 00h 01m | Losses/train_all_loss: 2.67e+00 (6.95e-01)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Skip RandomAffine for zero-area mask in first frame after 1 tentatives\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 2025-02-12 19:08:00,938 train_utils.py: 271: Train Epoch: [2][20/46] | Batch Time: 1.12 (1.24) | Data Time: 0.00 (0.29) | Mem (GB): 30.00 (28.81/30.00) | Time Elapsed: 00d 00h 02m | Losses/train_all_loss: 2.15e+00 (1.04e+00)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Skip RandomAffine for zero-area mask in first frame after 1 tentatives\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 2025-02-12 19:08:09,794 train_utils.py: 271: Train Epoch: [2][30/46] | Batch Time: 0.94 (1.12) | Data Time: 0.00 (0.19) | Mem (GB): 29.00 (28.71/30.00) | Time Elapsed: 00d 00h 02m | Losses/train_all_loss: 7.46e-01 (1.13e+00)\n",
      "INFO 2025-02-12 19:08:18,923 train_utils.py: 271: Train Epoch: [2][40/46] | Batch Time: 0.75 (1.07) | Data Time: 0.00 (0.15) | Mem (GB): 28.00 (28.68/30.00) | Time Elapsed: 00d 00h 02m | Losses/train_all_loss: 4.96e-02 (1.16e+00)\n",
      "INFO 2025-02-12 19:08:24,826 trainer.py: 950: Estimated time remaining: 00d 00h 13m\n",
      "INFO 2025-02-12 19:08:24,828 trainer.py: 892: Synchronizing meters\n",
      "INFO 2025-02-12 19:08:24,829 trainer.py: 830: Losses and meters: {'Losses/train_all_loss': 1.168295837452878, 'Losses/train_all_loss_mask': 0.011670781789491783, 'Losses/train_all_loss_dice': 0.5577969525171362, 'Losses/train_all_loss_iou': 0.255521372810978, 'Losses/train_all_loss_class': 0.12156187323990292, 'Losses/train_all_core_loss': 1.168295837452878, 'Trainer/where': 0.14891304347826087, 'Trainer/epoch': 2, 'Trainer/steps_train': 138}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Skip RandomAffine for zero-area mask in first frame after 1 tentatives\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 2025-02-12 19:08:31,792 train_utils.py: 271: Train Epoch: [3][ 0/46] | Batch Time: 5.60 (5.60) | Data Time: 4.43 (4.43) | Mem (GB): 30.00 (30.00/30.00) | Time Elapsed: 00d 00h 02m | Losses/train_all_loss: 3.78e+00 (3.78e+00)\n",
      "INFO 2025-02-12 19:08:41,006 train_utils.py: 271: Train Epoch: [3][10/46] | Batch Time: 0.97 (1.35) | Data Time: 0.00 (0.40) | Mem (GB): 29.00 (28.73/30.00) | Time Elapsed: 00d 00h 02m | Losses/train_all_loss: 3.67e+00 (1.25e+00)\n",
      "INFO 2025-02-12 19:08:50,617 train_utils.py: 271: Train Epoch: [3][20/46] | Batch Time: 0.76 (1.16) | Data Time: 0.00 (0.21) | Mem (GB): 28.00 (28.71/30.00) | Time Elapsed: 00d 00h 02m | Losses/train_all_loss: 1.81e-01 (1.28e+00)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Skip RandomAffine for zero-area mask in first frame after 1 tentatives\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 2025-02-12 19:08:59,531 train_utils.py: 271: Train Epoch: [3][30/46] | Batch Time: 1.17 (1.08) | Data Time: 0.00 (0.14) | Mem (GB): 30.00 (28.65/30.00) | Time Elapsed: 00d 00h 03m | Losses/train_all_loss: 4.10e-01 (1.11e+00)\n",
      "INFO 2025-02-12 19:09:09,285 train_utils.py: 271: Train Epoch: [3][40/46] | Batch Time: 0.83 (1.05) | Data Time: 0.00 (0.11) | Mem (GB): 28.00 (28.68/30.00) | Time Elapsed: 00d 00h 03m | Losses/train_all_loss: 7.40e-02 (9.95e-01)\n",
      "INFO 2025-02-12 19:09:14,711 trainer.py: 950: Estimated time remaining: 00d 00h 12m\n",
      "INFO 2025-02-12 19:09:14,713 trainer.py: 892: Synchronizing meters\n",
      "INFO 2025-02-12 19:09:14,714 trainer.py: 830: Losses and meters: {'Losses/train_all_loss': 1.0709656847883826, 'Losses/train_all_loss_mask': 0.007126032660143328, 'Losses/train_all_loss_dice': 0.5639036129350248, 'Losses/train_all_loss_iou': 0.2817036507286779, 'Losses/train_all_loss_class': 0.08283778894389363, 'Losses/train_all_core_loss': 1.0709656847883826, 'Trainer/where': 0.19891304347826086, 'Trainer/epoch': 3, 'Trainer/steps_train': 184}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Skip RandomAffine for zero-area mask in first frame after 1 tentatives\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 2025-02-12 19:09:21,565 train_utils.py: 271: Train Epoch: [4][ 0/46] | Batch Time: 5.51 (5.51) | Data Time: 4.66 (4.66) | Mem (GB): 28.00 (28.00/28.00) | Time Elapsed: 00d 00h 03m | Losses/train_all_loss: 2.77e-01 (2.77e-01)\n",
      "INFO 2025-02-12 19:09:31,294 train_utils.py: 271: Train Epoch: [4][10/46] | Batch Time: 1.18 (1.38) | Data Time: 0.00 (0.45) | Mem (GB): 29.00 (28.55/30.00) | Time Elapsed: 00d 00h 03m | Losses/train_all_loss: 3.04e-01 (9.18e-01)\n",
      "INFO 2025-02-12 19:09:41,329 train_utils.py: 271: Train Epoch: [4][20/46] | Batch Time: 1.16 (1.20) | Data Time: 0.00 (0.23) | Mem (GB): 29.00 (28.76/30.00) | Time Elapsed: 00d 00h 03m | Losses/train_all_loss: 2.79e+00 (1.14e+00)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Skip RandomAffine for zero-area mask in first frame after 1 tentatives\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 2025-02-12 19:09:50,721 train_utils.py: 271: Train Epoch: [4][30/46] | Batch Time: 1.12 (1.12) | Data Time: 0.00 (0.16) | Mem (GB): 30.00 (28.74/30.00) | Time Elapsed: 00d 00h 03m | Losses/train_all_loss: 2.46e+00 (1.26e+00)\n",
      "INFO 2025-02-12 19:09:59,213 train_utils.py: 271: Train Epoch: [4][40/46] | Batch Time: 0.97 (1.05) | Data Time: 0.00 (0.12) | Mem (GB): 29.00 (28.66/30.00) | Time Elapsed: 00d 00h 04m | Losses/train_all_loss: 1.33e+00 (1.18e+00)\n",
      "INFO 2025-02-12 19:10:05,220 trainer.py: 950: Estimated time remaining: 00d 00h 12m\n",
      "INFO 2025-02-12 19:10:05,223 trainer.py: 892: Synchronizing meters\n",
      "INFO 2025-02-12 19:10:05,223 trainer.py: 830: Losses and meters: {'Losses/train_all_loss': 1.1314884902018567, 'Losses/train_all_loss_mask': 0.007317020718840902, 'Losses/train_all_loss_dice': 0.6268343484920004, 'Losses/train_all_loss_iou': 0.28825435957506945, 'Losses/train_all_loss_class': 0.07005937089349137, 'Losses/train_all_core_loss': 1.1314884902018567, 'Trainer/where': 0.24891304347826088, 'Trainer/epoch': 4, 'Trainer/steps_train': 230}\n",
      "INFO 2025-02-12 19:10:11,817 train_utils.py: 271: Train Epoch: [5][ 0/46] | Batch Time: 5.22 (5.22) | Data Time: 4.36 (4.36) | Mem (GB): 28.00 (28.00/28.00) | Time Elapsed: 00d 00h 04m | Losses/train_all_loss: 3.35e-01 (3.35e-01)\n",
      "INFO 2025-02-12 19:10:22,239 train_utils.py: 271: Train Epoch: [5][10/46] | Batch Time: 1.22 (1.42) | Data Time: 0.00 (0.43) | Mem (GB): 30.00 (28.82/30.00) | Time Elapsed: 00d 00h 04m | Losses/train_all_loss: 2.63e-01 (1.21e+00)\n",
      "INFO 2025-02-12 19:10:32,986 train_utils.py: 271: Train Epoch: [5][20/46] | Batch Time: 1.24 (1.26) | Data Time: 0.00 (0.23) | Mem (GB): 30.00 (29.00/30.00) | Time Elapsed: 00d 00h 04m | Losses/train_all_loss: 1.10e+00 (1.12e+00)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Skip RandomAffine for zero-area mask in first frame after 1 tentatives\n",
      "WARNING:root:Skip RandomAffine for zero-area mask in first frame after 1 tentatives\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 2025-02-12 19:10:42,817 train_utils.py: 271: Train Epoch: [5][30/46] | Batch Time: 0.78 (1.17) | Data Time: 0.00 (0.15) | Mem (GB): 28.00 (28.84/30.00) | Time Elapsed: 00d 00h 04m | Losses/train_all_loss: 1.83e-01 (1.01e+00)\n",
      "INFO 2025-02-12 19:10:51,379 train_utils.py: 271: Train Epoch: [5][40/46] | Batch Time: 0.98 (1.09) | Data Time: 0.00 (0.12) | Mem (GB): 29.00 (28.71/30.00) | Time Elapsed: 00d 00h 05m | Losses/train_all_loss: 1.69e-01 (9.00e-01)\n",
      "INFO 2025-02-12 19:10:57,327 trainer.py: 950: Estimated time remaining: 00d 00h 11m\n",
      "INFO 2025-02-12 19:10:57,330 trainer.py: 892: Synchronizing meters\n",
      "INFO 2025-02-12 19:10:57,330 trainer.py: 830: Losses and meters: {'Losses/train_all_loss': 0.9802756623729415, 'Losses/train_all_loss_mask': 0.007126330775426964, 'Losses/train_all_loss_dice': 0.5520999263162198, 'Losses/train_all_loss_iou': 0.22004301807559704, 'Losses/train_all_loss_class': 0.06560610019209889, 'Losses/train_all_core_loss': 0.9802756623729415, 'Trainer/where': 0.29891304347826086, 'Trainer/epoch': 5, 'Trainer/steps_train': 276}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Skip RandomAffine for zero-area mask in first frame after 1 tentatives\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 2025-02-12 19:11:05,704 train_utils.py: 271: Train Epoch: [6][ 0/46] | Batch Time: 6.98 (6.98) | Data Time: 5.77 (5.77) | Mem (GB): 29.00 (29.00/29.00) | Time Elapsed: 00d 00h 05m | Losses/train_all_loss: 1.26e+00 (1.26e+00)\n",
      "INFO 2025-02-12 19:11:14,874 train_utils.py: 271: Train Epoch: [6][10/46] | Batch Time: 0.97 (1.47) | Data Time: 0.00 (0.53) | Mem (GB): 29.00 (28.73/30.00) | Time Elapsed: 00d 00h 05m | Losses/train_all_loss: 1.20e+00 (8.59e-01)\n",
      "INFO 2025-02-12 19:11:25,332 train_utils.py: 271: Train Epoch: [6][20/46] | Batch Time: 0.82 (1.27) | Data Time: 0.00 (0.28) | Mem (GB): 28.00 (28.86/30.00) | Time Elapsed: 00d 00h 05m | Losses/train_all_loss: 5.69e-01 (1.09e+00)\n",
      "INFO 2025-02-12 19:11:34,954 train_utils.py: 271: Train Epoch: [6][30/46] | Batch Time: 1.17 (1.17) | Data Time: 0.00 (0.19) | Mem (GB): 30.00 (28.84/30.00) | Time Elapsed: 00d 00h 05m | Losses/train_all_loss: 1.44e+00 (1.03e+00)\n",
      "INFO 2025-02-12 19:11:44,040 train_utils.py: 271: Train Epoch: [6][40/46] | Batch Time: 0.80 (1.11) | Data Time: 0.00 (0.14) | Mem (GB): 28.00 (28.76/30.00) | Time Elapsed: 00d 00h 05m | Losses/train_all_loss: 5.04e-01 (1.03e+00)\n",
      "INFO 2025-02-12 19:11:49,214 trainer.py: 950: Estimated time remaining: 00d 00h 10m\n",
      "INFO 2025-02-12 19:11:49,216 trainer.py: 892: Synchronizing meters\n",
      "INFO 2025-02-12 19:11:49,217 trainer.py: 830: Losses and meters: {'Losses/train_all_loss': 1.0356813777724039, 'Losses/train_all_loss_mask': 0.008182784920474818, 'Losses/train_all_loss_dice': 0.570926668851272, 'Losses/train_all_loss_iou': 0.2036640608035352, 'Losses/train_all_loss_class': 0.09743497043390532, 'Losses/train_all_core_loss': 1.0356813777724039, 'Trainer/where': 0.3489130434782609, 'Trainer/epoch': 6, 'Trainer/steps_train': 322}\n",
      "INFO 2025-02-12 19:11:57,098 train_utils.py: 271: Train Epoch: [7][ 0/46] | Batch Time: 6.52 (6.52) | Data Time: 5.72 (5.72) | Mem (GB): 28.00 (28.00/28.00) | Time Elapsed: 00d 00h 06m | Losses/train_all_loss: 7.90e-02 (7.90e-02)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Skip RandomAffine for zero-area mask in first frame after 1 tentatives\n",
      "WARNING:root:Skip RandomAffine for zero-area mask in first frame after 1 tentatives\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 2025-02-12 19:12:05,543 train_utils.py: 271: Train Epoch: [7][10/46] | Batch Time: 0.80 (1.36) | Data Time: 0.00 (0.52) | Mem (GB): 28.00 (28.18/30.00) | Time Elapsed: 00d 00h 06m | Losses/train_all_loss: 3.20e-01 (6.42e-01)\n",
      "INFO 2025-02-12 19:12:15,505 train_utils.py: 271: Train Epoch: [7][20/46] | Batch Time: 1.13 (1.19) | Data Time: 0.00 (0.27) | Mem (GB): 30.00 (28.48/30.00) | Time Elapsed: 00d 00h 06m | Losses/train_all_loss: 1.54e+00 (8.69e-01)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Skip RandomAffine for zero-area mask in first frame after 1 tentatives\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 2025-02-12 19:12:25,302 train_utils.py: 271: Train Epoch: [7][30/46] | Batch Time: 1.20 (1.12) | Data Time: 0.00 (0.19) | Mem (GB): 30.00 (28.58/30.00) | Time Elapsed: 00d 00h 06m | Losses/train_all_loss: 5.99e-01 (9.68e-01)\n",
      "INFO 2025-02-12 19:12:34,364 train_utils.py: 271: Train Epoch: [7][40/46] | Batch Time: 1.22 (1.07) | Data Time: 0.00 (0.14) | Mem (GB): 29.00 (28.54/30.00) | Time Elapsed: 00d 00h 06m | Losses/train_all_loss: 1.09e+00 (8.64e-01)\n",
      "INFO 2025-02-12 19:12:39,686 trainer.py: 950: Estimated time remaining: 00d 00h 09m\n",
      "INFO 2025-02-12 19:12:39,688 trainer.py: 892: Synchronizing meters\n",
      "INFO 2025-02-12 19:12:39,689 trainer.py: 830: Losses and meters: {'Losses/train_all_loss': 0.8107053305467834, 'Losses/train_all_loss_mask': 0.007405927927364879, 'Losses/train_all_loss_dice': 0.39357533143914264, 'Losses/train_all_loss_iou': 0.2286069194138374, 'Losses/train_all_loss_class': 0.04040452623300439, 'Losses/train_all_core_loss': 0.8107053305467834, 'Trainer/where': 0.3989130434782609, 'Trainer/epoch': 7, 'Trainer/steps_train': 368}\n",
      "INFO 2025-02-12 19:12:47,586 train_utils.py: 271: Train Epoch: [8][ 0/46] | Batch Time: 6.55 (6.55) | Data Time: 5.72 (5.72) | Mem (GB): 28.00 (28.00/28.00) | Time Elapsed: 00d 00h 06m | Losses/train_all_loss: 5.14e-02 (5.14e-02)\n",
      "INFO 2025-02-12 19:12:56,756 train_utils.py: 271: Train Epoch: [8][10/46] | Batch Time: 0.82 (1.43) | Data Time: 0.00 (0.52) | Mem (GB): 28.00 (28.55/30.00) | Time Elapsed: 00d 00h 07m | Losses/train_all_loss: 7.98e-01 (2.03e+00)\n",
      "INFO 2025-02-12 19:13:06,130 train_utils.py: 271: Train Epoch: [8][20/46] | Batch Time: 1.09 (1.19) | Data Time: 0.00 (0.27) | Mem (GB): 29.00 (28.52/30.00) | Time Elapsed: 00d 00h 07m | Losses/train_all_loss: 8.54e-01 (1.39e+00)\n",
      "INFO 2025-02-12 19:13:15,552 train_utils.py: 271: Train Epoch: [8][30/46] | Batch Time: 1.15 (1.11) | Data Time: 0.00 (0.19) | Mem (GB): 29.00 (28.55/30.00) | Time Elapsed: 00d 00h 07m | Losses/train_all_loss: 1.70e+00 (1.16e+00)\n",
      "INFO 2025-02-12 19:13:25,501 train_utils.py: 271: Train Epoch: [8][40/46] | Batch Time: 1.02 (1.08) | Data Time: 0.00 (0.14) | Mem (GB): 29.00 (28.61/30.00) | Time Elapsed: 00d 00h 07m | Losses/train_all_loss: 6.81e-01 (1.08e+00)\n",
      "INFO 2025-02-12 19:13:30,989 trainer.py: 950: Estimated time remaining: 00d 00h 09m\n",
      "INFO 2025-02-12 19:13:30,992 trainer.py: 892: Synchronizing meters\n",
      "INFO 2025-02-12 19:13:30,993 trainer.py: 830: Losses and meters: {'Losses/train_all_loss': 1.0143948501866797, 'Losses/train_all_loss_mask': 0.005940100329968593, 'Losses/train_all_loss_dice': 0.42324814329976623, 'Losses/train_all_loss_iou': 0.15048911871955448, 'Losses/train_all_loss_class': 0.321855566697632, 'Losses/train_all_core_loss': 1.0143948501866797, 'Trainer/where': 0.44891304347826083, 'Trainer/epoch': 8, 'Trainer/steps_train': 414}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Skip RandomAffine for zero-area mask in first frame after 1 tentatives\n",
      "WARNING:root:Skip RandomAffine for zero-area mask in first frame after 1 tentatives\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 2025-02-12 19:13:37,877 train_utils.py: 271: Train Epoch: [9][ 0/46] | Batch Time: 5.51 (5.51) | Data Time: 4.66 (4.66) | Mem (GB): 28.00 (28.00/28.00) | Time Elapsed: 00d 00h 07m | Losses/train_all_loss: 4.36e-01 (4.36e-01)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Skip RandomAffine for zero-area mask in first frame after 1 tentatives\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 2025-02-12 19:13:46,794 train_utils.py: 271: Train Epoch: [9][10/46] | Batch Time: 0.79 (1.31) | Data Time: 0.00 (0.42) | Mem (GB): 28.00 (28.45/30.00) | Time Elapsed: 00d 00h 07m | Losses/train_all_loss: 1.11e-01 (5.88e-01)\n",
      "INFO 2025-02-12 19:13:56,437 train_utils.py: 271: Train Epoch: [9][20/46] | Batch Time: 1.13 (1.15) | Data Time: 0.00 (0.22) | Mem (GB): 29.00 (28.52/30.00) | Time Elapsed: 00d 00h 08m | Losses/train_all_loss: 2.52e+00 (1.04e+00)\n",
      "INFO 2025-02-12 19:14:06,858 train_utils.py: 271: Train Epoch: [9][30/46] | Batch Time: 1.12 (1.11) | Data Time: 0.00 (0.15) | Mem (GB): 30.00 (28.68/30.00) | Time Elapsed: 00d 00h 08m | Losses/train_all_loss: 1.67e+00 (1.02e+00)\n",
      "INFO 2025-02-12 19:14:16,102 train_utils.py: 271: Train Epoch: [9][40/46] | Batch Time: 0.79 (1.07) | Data Time: 0.00 (0.11) | Mem (GB): 28.00 (28.61/30.00) | Time Elapsed: 00d 00h 08m | Losses/train_all_loss: 4.89e-01 (1.00e+00)\n",
      "INFO 2025-02-12 19:14:21,788 trainer.py: 950: Estimated time remaining: 00d 00h 08m\n",
      "INFO 2025-02-12 19:14:21,790 trainer.py: 892: Synchronizing meters\n",
      "INFO 2025-02-12 19:14:21,791 trainer.py: 830: Losses and meters: {'Losses/train_all_loss': 1.0212296913175478, 'Losses/train_all_loss_mask': 0.006365182768406711, 'Losses/train_all_loss_dice': 0.4835277925366941, 'Losses/train_all_loss_iou': 0.22865427034380642, 'Losses/train_all_loss_class': 0.1817439731583211, 'Losses/train_all_core_loss': 1.0212296913175478, 'Trainer/where': 0.4989130434782608, 'Trainer/epoch': 9, 'Trainer/steps_train': 460}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Skip RandomAffine for zero-area mask in first frame after 1 tentatives\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 2025-02-12 19:14:29,483 train_utils.py: 271: Train Epoch: [10][ 0/46] | Batch Time: 6.32 (6.32) | Data Time: 5.49 (5.49) | Mem (GB): 28.00 (28.00/28.00) | Time Elapsed: 00d 00h 08m | Losses/train_all_loss: 8.22e-02 (8.22e-02)\n",
      "INFO 2025-02-12 19:14:39,349 train_utils.py: 271: Train Epoch: [10][10/46] | Batch Time: 1.14 (1.47) | Data Time: 0.00 (0.50) | Mem (GB): 29.00 (28.64/30.00) | Time Elapsed: 00d 00h 08m | Losses/train_all_loss: 1.15e+00 (8.15e-01)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Skip RandomAffine for zero-area mask in first frame after 1 tentatives\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 2025-02-12 19:14:48,431 train_utils.py: 271: Train Epoch: [10][20/46] | Batch Time: 0.77 (1.20) | Data Time: 0.00 (0.26) | Mem (GB): 28.00 (28.52/30.00) | Time Elapsed: 00d 00h 08m | Losses/train_all_loss: 1.42e-01 (7.10e-01)\n",
      "INFO 2025-02-12 19:14:58,545 train_utils.py: 271: Train Epoch: [10][30/46] | Batch Time: 1.13 (1.14) | Data Time: 0.00 (0.18) | Mem (GB): 30.00 (28.71/30.00) | Time Elapsed: 00d 00h 09m | Losses/train_all_loss: 1.57e+00 (8.53e-01)\n",
      "INFO 2025-02-12 19:15:07,682 train_utils.py: 271: Train Epoch: [10][40/46] | Batch Time: 1.18 (1.09) | Data Time: 0.00 (0.13) | Mem (GB): 30.00 (28.66/30.00) | Time Elapsed: 00d 00h 09m | Losses/train_all_loss: 1.39e+00 (8.96e-01)\n",
      "INFO 2025-02-12 19:15:12,928 trainer.py: 950: Estimated time remaining: 00d 00h 07m\n",
      "INFO 2025-02-12 19:15:12,930 trainer.py: 892: Synchronizing meters\n",
      "INFO 2025-02-12 19:15:12,931 trainer.py: 830: Losses and meters: {'Losses/train_all_loss': 0.8961702939932761, 'Losses/train_all_loss_mask': 0.007089140032210549, 'Losses/train_all_loss_dice': 0.44686456866886304, 'Losses/train_all_loss_iou': 0.18015230983576697, 'Losses/train_all_loss_class': 0.12737062794719348, 'Losses/train_all_core_loss': 0.8961702939932761, 'Trainer/where': 0.5489130434782609, 'Trainer/epoch': 10, 'Trainer/steps_train': 506}\n",
      "INFO 2025-02-12 19:15:21,039 train_utils.py: 271: Train Epoch: [11][ 0/46] | Batch Time: 6.78 (6.78) | Data Time: 5.59 (5.59) | Mem (GB): 29.00 (29.00/29.00) | Time Elapsed: 00d 00h 09m | Losses/train_all_loss: 7.76e-01 (7.76e-01)\n",
      "INFO 2025-02-12 19:15:30,105 train_utils.py: 271: Train Epoch: [11][10/46] | Batch Time: 0.82 (1.44) | Data Time: 0.00 (0.51) | Mem (GB): 28.00 (28.55/30.00) | Time Elapsed: 00d 00h 09m | Losses/train_all_loss: 3.81e-01 (4.52e-01)\n",
      "INFO 2025-02-12 19:15:39,041 train_utils.py: 271: Train Epoch: [11][20/46] | Batch Time: 0.79 (1.18) | Data Time: 0.00 (0.27) | Mem (GB): 28.00 (28.48/30.00) | Time Elapsed: 00d 00h 09m | Losses/train_all_loss: 3.92e-01 (5.72e-01)\n",
      "INFO 2025-02-12 19:15:49,444 train_utils.py: 271: Train Epoch: [11][30/46] | Batch Time: 1.16 (1.13) | Data Time: 0.00 (0.18) | Mem (GB): 30.00 (28.65/30.00) | Time Elapsed: 00d 00h 09m | Losses/train_all_loss: 1.03e+00 (8.36e-01)\n",
      "INFO 2025-02-12 19:15:59,355 train_utils.py: 271: Train Epoch: [11][40/46] | Batch Time: 0.82 (1.10) | Data Time: 0.00 (0.14) | Mem (GB): 28.00 (28.71/30.00) | Time Elapsed: 00d 00h 10m | Losses/train_all_loss: 1.67e-01 (7.89e-01)\n",
      "INFO 2025-02-12 19:16:04,897 trainer.py: 950: Estimated time remaining: 00d 00h 06m\n",
      "INFO 2025-02-12 19:16:04,899 trainer.py: 892: Synchronizing meters\n",
      "INFO 2025-02-12 19:16:04,899 trainer.py: 830: Losses and meters: {'Losses/train_all_loss': 0.82965079192882, 'Losses/train_all_loss_mask': 0.009885185546527677, 'Losses/train_all_loss_dice': 0.37300079801808234, 'Losses/train_all_loss_iou': 0.19811731789504056, 'Losses/train_all_loss_class': 0.06082895174034528, 'Losses/train_all_core_loss': 0.82965079192882, 'Trainer/where': 0.5989130434782608, 'Trainer/epoch': 11, 'Trainer/steps_train': 552}\n",
      "INFO 2025-02-12 19:16:12,376 train_utils.py: 271: Train Epoch: [12][ 0/46] | Batch Time: 6.04 (6.04) | Data Time: 5.23 (5.23) | Mem (GB): 28.00 (28.00/28.00) | Time Elapsed: 00d 00h 10m | Losses/train_all_loss: 2.80e-01 (2.80e-01)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Skip RandomAffine for zero-area mask in first frame after 1 tentatives\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 2025-02-12 19:16:22,355 train_utils.py: 271: Train Epoch: [12][10/46] | Batch Time: 1.15 (1.46) | Data Time: 0.00 (0.48) | Mem (GB): 29.00 (28.55/29.00) | Time Elapsed: 00d 00h 10m | Losses/train_all_loss: 2.89e+00 (8.70e-01)\n",
      "INFO 2025-02-12 19:16:32,112 train_utils.py: 271: Train Epoch: [12][20/46] | Batch Time: 0.79 (1.23) | Data Time: 0.00 (0.25) | Mem (GB): 28.00 (28.71/30.00) | Time Elapsed: 00d 00h 10m | Losses/train_all_loss: 8.45e-01 (1.10e+00)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Skip RandomAffine for zero-area mask in first frame after 1 tentatives\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 2025-02-12 19:16:42,085 train_utils.py: 271: Train Epoch: [12][30/46] | Batch Time: 0.98 (1.15) | Data Time: 0.00 (0.17) | Mem (GB): 29.00 (28.81/30.00) | Time Elapsed: 00d 00h 10m | Losses/train_all_loss: 2.06e+00 (1.09e+00)\n",
      "INFO 2025-02-12 19:16:52,094 train_utils.py: 271: Train Epoch: [12][40/46] | Batch Time: 1.10 (1.12) | Data Time: 0.00 (0.13) | Mem (GB): 30.00 (28.90/30.00) | Time Elapsed: 00d 00h 11m | Losses/train_all_loss: 1.68e+00 (1.06e+00)\n",
      "INFO 2025-02-12 19:16:57,377 trainer.py: 950: Estimated time remaining: 00d 00h 05m\n",
      "INFO 2025-02-12 19:16:57,379 trainer.py: 892: Synchronizing meters\n",
      "INFO 2025-02-12 19:16:57,380 trainer.py: 830: Losses and meters: {'Losses/train_all_loss': 1.009698307060677, 'Losses/train_all_loss_mask': 0.009370061628390429, 'Losses/train_all_loss_dice': 0.5178368182285972, 'Losses/train_all_loss_iou': 0.26190884259488917, 'Losses/train_all_loss_class': 0.04255140851390076, 'Losses/train_all_core_loss': 1.009698307060677, 'Trainer/where': 0.6489130434782608, 'Trainer/epoch': 12, 'Trainer/steps_train': 598}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Skip RandomAffine for zero-area mask in first frame after 1 tentatives\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 2025-02-12 19:17:04,347 train_utils.py: 271: Train Epoch: [13][ 0/46] | Batch Time: 5.54 (5.54) | Data Time: 4.71 (4.71) | Mem (GB): 28.00 (28.00/28.00) | Time Elapsed: 00d 00h 11m | Losses/train_all_loss: 9.48e-02 (9.48e-02)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Skip RandomAffine for zero-area mask in first frame after 1 tentatives\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 2025-02-12 19:17:14,280 train_utils.py: 271: Train Epoch: [13][10/46] | Batch Time: 0.78 (1.41) | Data Time: 0.00 (0.46) | Mem (GB): 28.00 (28.45/29.00) | Time Elapsed: 00d 00h 11m | Losses/train_all_loss: 2.85e-01 (9.72e-01)\n",
      "INFO 2025-02-12 19:17:23,357 train_utils.py: 271: Train Epoch: [13][20/46] | Batch Time: 1.15 (1.17) | Data Time: 0.00 (0.24) | Mem (GB): 30.00 (28.52/30.00) | Time Elapsed: 00d 00h 11m | Losses/train_all_loss: 8.23e-01 (1.16e+00)\n",
      "INFO 2025-02-12 19:17:32,451 train_utils.py: 271: Train Epoch: [13][30/46] | Batch Time: 0.80 (1.09) | Data Time: 0.00 (0.16) | Mem (GB): 28.00 (28.52/30.00) | Time Elapsed: 00d 00h 11m | Losses/train_all_loss: 1.76e-01 (1.08e+00)\n",
      "INFO 2025-02-12 19:17:42,242 train_utils.py: 271: Train Epoch: [13][40/46] | Batch Time: 0.77 (1.06) | Data Time: 0.00 (0.12) | Mem (GB): 28.00 (28.63/30.00) | Time Elapsed: 00d 00h 11m | Losses/train_all_loss: 2.24e-01 (1.08e+00)\n",
      "INFO 2025-02-12 19:17:48,436 trainer.py: 950: Estimated time remaining: 00d 00h 04m\n",
      "INFO 2025-02-12 19:17:48,438 trainer.py: 892: Synchronizing meters\n",
      "INFO 2025-02-12 19:17:48,439 trainer.py: 830: Losses and meters: {'Losses/train_all_loss': 1.074780048640526, 'Losses/train_all_loss_mask': 0.0062736710843518785, 'Losses/train_all_loss_dice': 0.43303531667460565, 'Losses/train_all_loss_iou': 0.22974199441302082, 'Losses/train_all_loss_class': 0.2865293079579475, 'Losses/train_all_core_loss': 1.074780048640526, 'Trainer/where': 0.6989130434782609, 'Trainer/epoch': 13, 'Trainer/steps_train': 644}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Skip RandomAffine for zero-area mask in first frame after 1 tentatives\n",
      "WARNING:root:Skip RandomAffine for zero-area mask in first frame after 1 tentatives\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 2025-02-12 19:17:56,868 train_utils.py: 271: Train Epoch: [14][ 0/46] | Batch Time: 7.16 (7.16) | Data Time: 5.93 (5.93) | Mem (GB): 30.00 (30.00/30.00) | Time Elapsed: 00d 00h 12m | Losses/train_all_loss: 1.39e+00 (1.39e+00)\n",
      "INFO 2025-02-12 19:18:06,136 train_utils.py: 271: Train Epoch: [14][10/46] | Batch Time: 0.81 (1.49) | Data Time: 0.00 (0.54) | Mem (GB): 28.00 (28.64/30.00) | Time Elapsed: 00d 00h 12m | Losses/train_all_loss: 2.20e-01 (1.18e+00)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Skip RandomAffine for zero-area mask in first frame after 1 tentatives\n",
      "WARNING:root:Skip RandomAffine for zero-area mask in first frame after 1 tentatives\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 2025-02-12 19:18:15,879 train_utils.py: 271: Train Epoch: [14][20/46] | Batch Time: 0.80 (1.25) | Data Time: 0.00 (0.28) | Mem (GB): 28.00 (28.62/30.00) | Time Elapsed: 00d 00h 12m | Losses/train_all_loss: 7.14e-02 (1.02e+00)\n",
      "INFO 2025-02-12 19:18:25,213 train_utils.py: 271: Train Epoch: [14][30/46] | Batch Time: 0.80 (1.15) | Data Time: 0.00 (0.19) | Mem (GB): 28.00 (28.58/30.00) | Time Elapsed: 00d 00h 12m | Losses/train_all_loss: 4.83e-01 (1.00e+00)\n",
      "INFO 2025-02-12 19:18:34,537 train_utils.py: 271: Train Epoch: [14][40/46] | Batch Time: 1.13 (1.09) | Data Time: 0.00 (0.15) | Mem (GB): 30.00 (28.59/30.00) | Time Elapsed: 00d 00h 12m | Losses/train_all_loss: 5.89e-01 (1.10e+00)\n",
      "INFO 2025-02-12 19:18:40,248 trainer.py: 950: Estimated time remaining: 00d 00h 04m\n",
      "INFO 2025-02-12 19:18:40,251 trainer.py: 892: Synchronizing meters\n",
      "INFO 2025-02-12 19:18:40,251 trainer.py: 830: Losses and meters: {'Losses/train_all_loss': 1.0234140995889902, 'Losses/train_all_loss_mask': 0.005420750215821697, 'Losses/train_all_loss_dice': 0.5121119294477545, 'Losses/train_all_loss_iou': 0.23446821026585024, 'Losses/train_all_loss_class': 0.16841893756852006, 'Losses/train_all_core_loss': 1.0234140995889902, 'Trainer/where': 0.7489130434782608, 'Trainer/epoch': 14, 'Trainer/steps_train': 690}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Skip RandomAffine for zero-area mask in first frame after 1 tentatives\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 2025-02-12 19:18:48,155 train_utils.py: 271: Train Epoch: [15][ 0/46] | Batch Time: 6.56 (6.56) | Data Time: 5.75 (5.75) | Mem (GB): 28.00 (28.00/28.00) | Time Elapsed: 00d 00h 12m | Losses/train_all_loss: 1.27e+00 (1.27e+00)\n",
      "INFO 2025-02-12 19:18:57,199 train_utils.py: 271: Train Epoch: [15][10/46] | Batch Time: 1.11 (1.42) | Data Time: 0.00 (0.52) | Mem (GB): 29.00 (28.45/29.00) | Time Elapsed: 00d 00h 13m | Losses/train_all_loss: 1.46e+00 (7.66e-01)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Skip RandomAffine for zero-area mask in first frame after 1 tentatives\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 2025-02-12 19:19:06,053 train_utils.py: 271: Train Epoch: [15][20/46] | Batch Time: 0.77 (1.16) | Data Time: 0.00 (0.27) | Mem (GB): 28.00 (28.48/30.00) | Time Elapsed: 00d 00h 13m | Losses/train_all_loss: 8.58e-02 (7.77e-01)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Skip RandomAffine for zero-area mask in first frame after 1 tentatives\n",
      "WARNING:root:Skip RandomAffine for zero-area mask in first frame after 1 tentatives\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 2025-02-12 19:19:15,457 train_utils.py: 271: Train Epoch: [15][30/46] | Batch Time: 0.80 (1.09) | Data Time: 0.00 (0.19) | Mem (GB): 28.00 (28.55/30.00) | Time Elapsed: 00d 00h 13m | Losses/train_all_loss: 2.54e-01 (7.73e-01)\n",
      "INFO 2025-02-12 19:19:24,905 train_utils.py: 271: Train Epoch: [15][40/46] | Batch Time: 0.79 (1.06) | Data Time: 0.00 (0.14) | Mem (GB): 28.00 (28.59/30.00) | Time Elapsed: 00d 00h 13m | Losses/train_all_loss: 1.56e-01 (7.50e-01)\n",
      "INFO 2025-02-12 19:19:30,691 trainer.py: 950: Estimated time remaining: 00d 00h 03m\n",
      "INFO 2025-02-12 19:19:30,694 trainer.py: 892: Synchronizing meters\n",
      "INFO 2025-02-12 19:19:30,695 trainer.py: 830: Losses and meters: {'Losses/train_all_loss': 0.7267211427831132, 'Losses/train_all_loss_mask': 0.007178973572318564, 'Losses/train_all_loss_dice': 0.3941910409409067, 'Losses/train_all_loss_iou': 0.15980009955313543, 'Losses/train_all_loss_class': 0.029150538045254602, 'Losses/train_all_core_loss': 0.7267211427831132, 'Trainer/where': 0.7989130434782609, 'Trainer/epoch': 15, 'Trainer/steps_train': 736}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Skip RandomAffine for zero-area mask in first frame after 1 tentatives\n",
      "WARNING:root:Skip RandomAffine for zero-area mask in first frame after 1 tentatives\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 2025-02-12 19:19:37,749 train_utils.py: 271: Train Epoch: [16][ 0/46] | Batch Time: 5.77 (5.77) | Data Time: 4.54 (4.54) | Mem (GB): 29.00 (29.00/29.00) | Time Elapsed: 00d 00h 13m | Losses/train_all_loss: 2.97e+00 (2.97e+00)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Skip RandomAffine for zero-area mask in first frame after 1 tentatives\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 2025-02-12 19:19:47,131 train_utils.py: 271: Train Epoch: [16][10/46] | Batch Time: 0.77 (1.38) | Data Time: 0.00 (0.41) | Mem (GB): 28.00 (28.64/30.00) | Time Elapsed: 00d 00h 13m | Losses/train_all_loss: 3.83e-01 (1.20e+00)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Skip RandomAffine for zero-area mask in first frame after 1 tentatives\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 2025-02-12 19:19:55,955 train_utils.py: 271: Train Epoch: [16][20/46] | Batch Time: 0.86 (1.14) | Data Time: 0.00 (0.22) | Mem (GB): 28.00 (28.48/30.00) | Time Elapsed: 00d 00h 14m | Losses/train_all_loss: 1.80e-01 (8.94e-01)\n",
      "INFO 2025-02-12 19:20:04,985 train_utils.py: 271: Train Epoch: [16][30/46] | Batch Time: 1.15 (1.06) | Data Time: 0.00 (0.15) | Mem (GB): 30.00 (28.52/30.00) | Time Elapsed: 00d 00h 14m | Losses/train_all_loss: 1.03e+00 (7.87e-01)\n",
      "INFO 2025-02-12 19:20:14,313 train_utils.py: 271: Train Epoch: [16][40/46] | Batch Time: 0.85 (1.03) | Data Time: 0.00 (0.11) | Mem (GB): 28.00 (28.54/30.00) | Time Elapsed: 00d 00h 14m | Losses/train_all_loss: 7.36e-02 (7.48e-01)\n",
      "INFO 2025-02-12 19:20:20,250 trainer.py: 950: Estimated time remaining: 00d 00h 02m\n",
      "INFO 2025-02-12 19:20:20,252 trainer.py: 892: Synchronizing meters\n",
      "INFO 2025-02-12 19:20:20,253 trainer.py: 830: Losses and meters: {'Losses/train_all_loss': 0.7119199207619481, 'Losses/train_all_loss_mask': 0.005569321207789745, 'Losses/train_all_loss_dice': 0.41522900047509564, 'Losses/train_all_loss_iou': 0.1757143955151348, 'Losses/train_all_loss_class': 0.009590096424319752, 'Losses/train_all_core_loss': 0.7119199207619481, 'Trainer/where': 0.8489130434782609, 'Trainer/epoch': 16, 'Trainer/steps_train': 782}\n",
      "INFO 2025-02-12 19:20:27,980 train_utils.py: 271: Train Epoch: [17][ 0/46] | Batch Time: 6.24 (6.24) | Data Time: 5.40 (5.40) | Mem (GB): 28.00 (28.00/28.00) | Time Elapsed: 00d 00h 14m | Losses/train_all_loss: 3.08e-01 (3.08e-01)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Skip RandomAffine for zero-area mask in first frame after 1 tentatives\n",
      "WARNING:root:Skip RandomAffine for zero-area mask in first frame after 1 tentatives\n",
      "WARNING:root:Skip RandomAffine for zero-area mask in first frame after 1 tentatives\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 2025-02-12 19:20:37,480 train_utils.py: 271: Train Epoch: [17][10/46] | Batch Time: 0.80 (1.43) | Data Time: 0.00 (0.49) | Mem (GB): 28.00 (28.45/30.00) | Time Elapsed: 00d 00h 14m | Losses/train_all_loss: 4.30e-01 (5.70e-01)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Skip RandomAffine for zero-area mask in first frame after 1 tentatives\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 2025-02-12 19:20:46,715 train_utils.py: 271: Train Epoch: [17][20/46] | Batch Time: 0.79 (1.19) | Data Time: 0.00 (0.26) | Mem (GB): 28.00 (28.57/30.00) | Time Elapsed: 00d 00h 14m | Losses/train_all_loss: 1.25e-01 (5.75e-01)\n",
      "INFO 2025-02-12 19:20:56,966 train_utils.py: 271: Train Epoch: [17][30/46] | Batch Time: 0.99 (1.14) | Data Time: 0.00 (0.18) | Mem (GB): 29.00 (28.68/30.00) | Time Elapsed: 00d 00h 15m | Losses/train_all_loss: 6.05e-01 (6.59e-01)\n",
      "INFO 2025-02-12 19:21:06,228 train_utils.py: 271: Train Epoch: [17][40/46] | Batch Time: 0.81 (1.09) | Data Time: 0.00 (0.13) | Mem (GB): 28.00 (28.63/30.00) | Time Elapsed: 00d 00h 15m | Losses/train_all_loss: 1.63e-01 (6.14e-01)\n",
      "INFO 2025-02-12 19:21:11,671 trainer.py: 950: Estimated time remaining: 00d 00h 01m\n",
      "INFO 2025-02-12 19:21:11,673 trainer.py: 892: Synchronizing meters\n",
      "INFO 2025-02-12 19:21:11,674 trainer.py: 830: Losses and meters: {'Losses/train_all_loss': 0.6726862106310285, 'Losses/train_all_loss_mask': 0.00649114266865502, 'Losses/train_all_loss_dice': 0.37154946897340857, 'Losses/train_all_loss_iou': 0.16359300826392745, 'Losses/train_all_loss_class': 0.007720872682613774, 'Losses/train_all_core_loss': 0.6726862106310285, 'Trainer/where': 0.898913043478261, 'Trainer/epoch': 17, 'Trainer/steps_train': 828}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Skip RandomAffine for zero-area mask in first frame after 1 tentatives\n",
      "WARNING:root:Skip RandomAffine for zero-area mask in first frame after 1 tentatives\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 2025-02-12 19:21:20,022 train_utils.py: 271: Train Epoch: [18][ 0/46] | Batch Time: 6.94 (6.94) | Data Time: 5.75 (5.75) | Mem (GB): 30.00 (30.00/30.00) | Time Elapsed: 00d 00h 15m | Losses/train_all_loss: 8.10e-01 (8.10e-01)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Skip RandomAffine for zero-area mask in first frame after 1 tentatives\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 2025-02-12 19:21:29,152 train_utils.py: 271: Train Epoch: [18][10/46] | Batch Time: 0.78 (1.46) | Data Time: 0.00 (0.52) | Mem (GB): 28.00 (28.82/30.00) | Time Elapsed: 00d 00h 15m | Losses/train_all_loss: 5.20e-02 (8.76e-01)\n",
      "INFO 2025-02-12 19:21:39,246 train_utils.py: 271: Train Epoch: [18][20/46] | Batch Time: 1.17 (1.25) | Data Time: 0.00 (0.27) | Mem (GB): 30.00 (28.95/30.00) | Time Elapsed: 00d 00h 15m | Losses/train_all_loss: 4.24e-01 (1.10e+00)\n",
      "INFO 2025-02-12 19:21:48,205 train_utils.py: 271: Train Epoch: [18][30/46] | Batch Time: 0.80 (1.13) | Data Time: 0.00 (0.19) | Mem (GB): 28.00 (28.77/30.00) | Time Elapsed: 00d 00h 15m | Losses/train_all_loss: 7.33e-02 (8.95e-01)\n",
      "INFO 2025-02-12 19:21:58,114 train_utils.py: 271: Train Epoch: [18][40/46] | Batch Time: 0.80 (1.10) | Data Time: 0.00 (0.14) | Mem (GB): 28.00 (28.78/30.00) | Time Elapsed: 00d 00h 16m | Losses/train_all_loss: 3.47e-01 (9.73e-01)\n",
      "INFO 2025-02-12 19:22:03,798 trainer.py: 950: Estimated time remaining: 00d 00h 00m\n",
      "INFO 2025-02-12 19:22:03,801 trainer.py: 892: Synchronizing meters\n",
      "INFO 2025-02-12 19:22:03,801 trainer.py: 830: Losses and meters: {'Losses/train_all_loss': 0.9628484164891036, 'Losses/train_all_loss_mask': 0.009301467431768011, 'Losses/train_all_loss_dice': 0.47046321889628534, 'Losses/train_all_loss_iou': 0.20331681627051337, 'Losses/train_all_loss_class': 0.10303904204817062, 'Losses/train_all_core_loss': 0.9628484164891036, 'Trainer/where': 0.9489130434782609, 'Trainer/epoch': 18, 'Trainer/steps_train': 874}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Skip RandomAffine for zero-area mask in first frame after 1 tentatives\n",
      "WARNING:root:Skip RandomAffine for zero-area mask in first frame after 1 tentatives\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 2025-02-12 19:22:10,938 train_utils.py: 271: Train Epoch: [19][ 0/46] | Batch Time: 5.80 (5.80) | Data Time: 4.97 (4.97) | Mem (GB): 28.00 (28.00/28.00) | Time Elapsed: 00d 00h 16m | Losses/train_all_loss: 2.22e-01 (2.22e-01)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Skip RandomAffine for zero-area mask in first frame after 1 tentatives\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 2025-02-12 19:22:21,498 train_utils.py: 271: Train Epoch: [19][10/46] | Batch Time: 0.79 (1.49) | Data Time: 0.00 (0.45) | Mem (GB): 28.00 (29.00/30.00) | Time Elapsed: 00d 00h 16m | Losses/train_all_loss: 2.25e-01 (1.64e+00)\n",
      "INFO 2025-02-12 19:22:30,323 train_utils.py: 271: Train Epoch: [19][20/46] | Batch Time: 0.77 (1.20) | Data Time: 0.00 (0.24) | Mem (GB): 28.00 (28.71/30.00) | Time Elapsed: 00d 00h 16m | Losses/train_all_loss: 5.51e-01 (1.32e+00)\n",
      "INFO 2025-02-12 19:22:39,577 train_utils.py: 271: Train Epoch: [19][30/46] | Batch Time: 1.13 (1.11) | Data Time: 0.00 (0.16) | Mem (GB): 30.00 (28.71/30.00) | Time Elapsed: 00d 00h 16m | Losses/train_all_loss: 7.91e-01 (1.27e+00)\n",
      "INFO 2025-02-12 19:22:48,565 train_utils.py: 271: Train Epoch: [19][40/46] | Batch Time: 0.77 (1.06) | Data Time: 0.00 (0.12) | Mem (GB): 28.00 (28.68/30.00) | Time Elapsed: 00d 00h 16m | Losses/train_all_loss: 3.85e-02 (1.18e+00)\n",
      "INFO 2025-02-12 19:22:54,750 trainer.py: 950: Estimated time remaining: 00d 00h 00m\n",
      "INFO 2025-02-12 19:22:54,753 trainer.py: 892: Synchronizing meters\n",
      "INFO 2025-02-12 19:22:54,754 trainer.py: 830: Losses and meters: {'Losses/train_all_loss': 1.1466085773123347, 'Losses/train_all_loss_mask': 0.008743165644142245, 'Losses/train_all_loss_dice': 0.5724411321722943, 'Losses/train_all_loss_iou': 0.28790114911111153, 'Losses/train_all_loss_class': 0.11140297832145422, 'Losses/train_all_core_loss': 1.1466085773123347, 'Trainer/where': 0.9989130434782609, 'Trainer/epoch': 19, 'Trainer/steps_train': 920}\n"
     ]
    }
   ],
   "source": [
    "single_node_runner(cfg, 4500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finito\n"
     ]
    }
   ],
   "source": [
    "print(\"Finito\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
